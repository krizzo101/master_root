---
description: MUST verify test coverage WHEN running integration tests TO ensure comprehensive validation.
globs: ["**/*.test.*", "**/*.spec.*", "**/*test*.*", "**/*test*/**", "**/completeness/**", "**/integration/**", "**/test/**", "**/*completeness*/**", "**/*integration*/**"]
---
# integration-test-completeness

<version>1.0.0</version>

## Metadata
{
  "rule_id": "3702-integration-test-completeness",
  "taxonomy": {
    "category": "Testing Standards",
    "parent": "Testing StandardsRule",
    "ancestors": [
      "Rule",
      "Testing StandardsRule"
    ],
    "children": []
  },
  "tags": [
    "integration test",
    "test completeness",
    "quality assurance",
    "software testing"
  ],
  "priority": "75",
  "inherits": [
    "000-context-awareness",
    "001-ai-operating-rules"
  ]
}

## Overview
{
  "purpose": "The purpose of this rule is to ensure that all integration tests provide adequate coverage of the system's functionality, validating interactions between components.",
  "application": "This rule should be applied during the integration testing phase of the development lifecycle. It MUST be ensured that all integration tests are executed and reviewed for completeness, addressing various interaction scenarios.",
  "importance": "This rule is crucial as it helps identify potential defects in the interactions between different system components, thereby improving the overall reliability and robustness of the software. Incomplete integration testing could lead to undetected issues that manifest in production, ultimately affecting user satisfaction and trust."
}

## test_coverage

{
  "description": "This section outlines the requirements for ensuring comprehensive test coverage in integration tests.",
  "requirements": [
    "MUST include tests that cover all critical paths of interaction between components.",
    "MUST ensure that edge cases are tested to verify system stability under unusual conditions.",
    "SHOULD include tests for failure scenarios to assess system resilience."
  ]
}

## test_execution

{
  "description": "This section details the requirements for the execution of integration tests.",
  "requirements": [
    "MUST execute all integration tests on every relevant code change to ensure ongoing coverage.",
    "SHOULD report the results of integration tests clearly, including successes and failures for easy review.",
    "MUST automate the execution of integration tests as part of the continuous integration pipeline."
  ]
}

## test_review

{
  "description": "This section describes the requirements for reviewing integration tests for completeness and effectiveness.",
  "requirements": [
    "MUST conduct regular reviews of integration tests to identify any gaps in coverage.",
    "SHOULD involve multiple team members in the review process to gain diverse insights.",
    "MUST update tests promptly based on findings from the review process to maintain relevance."
  ]
}

<example>
integration-test-completeness Example

```python
# Example for integration-test-completeness
def example():
    # Implement according to standards
    pass
```

This example demonstrates how to implement integration-test-completeness according to the standards.
</example>

<danger>
{
  "critical_violations": [
    "NEVER skip testing critical paths of interaction between components in integration tests.",
    "NEVER overlook edge cases that could lead to system instability during integration testing.",
    "NEVER execute integration tests without ensuring that all relevant scenarios have been covered.",
    "NEVER ignore the results of integration test reviews, especially if gaps in coverage are identified.",
    "NEVER automate integration test execution without maintaining a clear record of test results and their implications."
  ],
  "specific_risks": [
    "Incomplete integration tests may lead to undetected defects in the interaction between components, causing failures in production.",
    "Failing to test edge cases could result in system crashes or unexpected behavior under unusual conditions, negatively impacting user experience.",
    "Neglecting to review test results could allow critical issues to persist, leading to significant technical debt and reduced software reliability.",
    "Overlooking the necessity of including failure scenarios may result in a lack of resilience in the system, increasing downtime and operational costs.",
    "Inadequate documentation of test execution may hinder troubleshooting efforts, making it difficult to diagnose and resolve issues when they arise."
  ]
}
</danger>
