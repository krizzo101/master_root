# EVALUATE WORKFLOW

keywords: [evaluate, assess, analyze, review, measure]
expectation: evidence_based_evaluation_with_metrics_and_recommendations
completion: comprehensive_assessment_documented_findings_actionable_insights

phases:
  context_assessment:
    tools: [mcp_cognitive_tools_arango_search, codebase_search, list_dir]
    validation: context_established
    steps:
      - scope_analysis → evaluation_request → [technical, functional, performance, quality, usability]
      - tool: mcp_cognitive_tools_arango_search
        params:
          search_type: content
          collection: research_docs
          content: evaluation_topics
          limit: 20
        validation: existing_evaluations_retrieved
      - baseline_establishment → current_state → evaluation_criteria → domain_metrics
      - framework_selection → evaluation_type → domain_methodologies
    gates: [scope_clear, context_established, baseline_documented, framework_selected]

  data_collection:
    tools: [read_file, codebase_search, grep_search, run_terminal_cmd, mcp_cognitive_tools_arango_search]
    parallel: true
    validation: comprehensive_data_collected
    steps:
      - tool: codebase_search
        params: {query: evaluation_target_components}
        validation: code_components_identified
      - tool: read_file
        params: {target_file: configuration_documentation_files}
        validation: documentation_reviewed
      - tool: grep_search
        params: {query: pattern_specific_searches, include_pattern: relevant_file_types}
        validation: usage_patterns_identified
      - tool: run_terminal_cmd
        params: {command: performance_testing_commands}
        validation: performance_metrics_collected
      - tool: mcp_cognitive_tools_arango_search
        params:
          search_type: content
          collection: metrics
          content: evaluation_target performance historical
        validation: historical_performance_retrieved
      - stakeholder_input_collection → user_feedback_requirements_constraints
    gates: [data_comprehensive, sources_multiple, metrics_quantified, perspectives_captured]

  analysis_synthesis:
    tools: [mcp_thinking_sequentialthinking, analysis_engine]
    validation: systematic_analysis_evidence_based
    steps:
      - tool: mcp_thinking_sequentialthinking
        params: {thought_process: systematic_evaluation_analysis, total_thoughts: 5}
        validation: structured_framework_applied
      - metric_analysis → quantitative_qualitative_data → trends_identified
      - comparative_analysis → baseline_standards_practices → current_system
      - gap_identification → current_vs_desired_state → gaps_prioritized
      - strength_weakness_analysis → comprehensive_data → evidence_documented
    gates: [analysis_systematic, conclusions_evidence_based, gaps_identified, strengths_clear]

  recommendation_development:
    tools: [recommendation_engine, mcp_cognitive_tools_arango_search]
    validation: actionable_recommendations_implementation_guidance
    steps:
      - recommendation_synthesis → analysis_findings + constraints → practical_actionable
      - tool: mcp_cognitive_tools_arango_search
        params:
          search_type: content
          collection: research_docs
          content: similar_recommendations_best_practices synthesis
          limit: 15
        validation: best_practices_incorporated
      - implementation_roadmap → prioritized_recommendations + resources → timelines
      - risk_assessment → proposed_changes → mitigation_strategies
      - success_metrics → implementation_plan → measurable_criteria
    gates: [recommendations_actionable, implementation_planned, risks_assessed, metrics_defined]

  documentation:
    tools: [mcp_cognitive_tools_arango_modify, documentation_engine]
    validation: comprehensive_evaluation_report_traceability
    steps:
      - enforcement_compliance_check: Load `.cursor/protocols/operational-enforcement.md` + validate systematic KB consultation completed
      - findings_synthesis → evaluation_data → comprehensive_report
      - evidence_compilation → metrics_examples_input → conclusions_documented
      - recommendation_prioritization → impact_effort_risk → rationale
      - tool: mcp_cognitive_tools_arango_modify
        params:
          operation: insert
          collection: research_docs
          document:
            type: evaluation_complete
            content_type: "evaluation"
            evaluation_target: evaluated_system_component
            findings: comprehensive_findings
            recommendations: prioritized_recommendations
            implementation_plan: detailed_roadmap
            success_metrics: measurable_criteria
            created: current_time
        validation: evaluation_stored_with_id
      - enforcement_tracking: Log evaluation completion in metrics collection per operational-enforcement protocol

failure_prevention:
  avoid:
    [
      superficial_analysis,
      single_source_evaluation,
      unsupported_conclusions,
      vague_recommendations,
      implementation_guidance_missing,
    ]
  require:
    [
      comprehensive_data_collection,
      evidence_based_analysis,
      actionable_recommendations,
      implementation_planning,
      success_metrics_defined,
    ]

success_validation:
  before_completion:
    [
      context_established,
      data_comprehensive,
      analysis_systematic,
      recommendations_actionable,
      evaluation_documented,
    ]
  output_requirements: stored_evaluation_report_prioritized_recommendations_implementation_plan_success_metrics

# Critical enforcement points
critical_mandatory:
  - comprehensive_context_assessment_before_data_collection
  - multiple_source_data_collection_for_thorough_evaluation
  - evidence_based_analysis_with_systematic_framework
  - actionable_recommendations_with_implementation_guidance
  - comprehensive_documentation_with_traceability

# Evaluation frameworks by domain
evaluation_frameworks:
  technical_systems:
    metrics: [performance, scalability, maintainability, security, reliability]
    methods: [code_review, performance_testing, security_analysis]
  user_interfaces:
    metrics: [usability, accessibility, user_satisfaction, efficiency]
    methods: [usability_testing, user_feedback, accessibility_audit]
  processes:
    metrics: [efficiency, effectiveness, compliance, quality, satisfaction]
    methods: [process_mapping, stakeholder_interviews, metric_analysis]
  knowledge_systems:
    metrics: [accuracy, completeness, accessibility, usability, maintenance]
    methods: [content_audit, user_feedback, usage_analytics]
