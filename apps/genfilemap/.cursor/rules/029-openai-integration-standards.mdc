---
description: WHEN integrating with OpenAI services TO ensure best practices, cost control, and reliability YOU MUST follow standardized patterns for authentication, model selection, API usage, and error handling
globs:
alwaysApply: false
---

{
"title": "OpenAI Integration Standards v3.2 - Enhanced Code Examples",
"comprehensive*code_examples": {
"depth": 1,
"trigger": "openai-implementation",
"complete_implementations": {
"python_basic_setup": {
"description": "Complete Python setup with Responses API",
"code": "import os\nimport asyncio\nfrom typing import Optional\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Configuration\nclass Config:\n OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n DEFAULT_MODEL = 'gpt-4.1-mini'\n FALLBACK_MODEL = 'gpt-4.1-nano'\n MAX_RETRIES = 3\n TIMEOUT = 30\n\n# Initialize client (reuse this instance)\nclient = OpenAI(\n api_key=Config.OPENAI_API_KEY,\n max_retries=Config.MAX_RETRIES,\n timeout=Config.TIMEOUT\n)\n\n# Basic response\nresponse = client.responses.create(\n model=Config.DEFAULT_MODEL,\n input='Hello, how are you?',\n instructions='You are a helpful assistant. Be concise.'\n)\nprint(response.output_text)\n\n# Access usage info\nusage = response.usage\nprint(f'Input tokens: {usage.input_tokens}, Output tokens: {usage.output_tokens}')"
},
"nodejs_basic_setup": {
"description": "Complete Node.js/TypeScript setup with Responses API",
"code": "import OpenAI from 'openai';\nimport { z } from 'zod';\nimport { zodResponseFormat } from 'openai/helpers/zod';\n\n// Configuration\nconst config = {\n apiKey: process.env.OPENAI_API_KEY,\n defaultModel: 'gpt-4.1-mini' as const,\n fallbackModel: 'gpt-4.1-nano' as const,\n maxRetries: 3,\n timeout: 30000\n};\n\n// Initialize client (reuse this instance)\nconst client = new OpenAI({\n apiKey: config.apiKey,\n maxRetries: config.maxRetries,\n timeout: config.timeout\n});\n\n// Basic response\nconst response = await client.responses.create({\n model: config.defaultModel,\n input: 'Hello, how are you?',\n instructions: 'You are a helpful assistant. Be concise.'\n});\nconsole.log(response.output_text);\n\n// Access usage info\nconst usage = response.usage;\nconsole.log(`Input tokens: ${usage.input_tokens}, Output tokens: ${usage.output_tokens}`);"
},
"python_structured_outputs": {
"description": "Python structured outputs with Pydantic",
"code": "from pydantic import BaseModel\nfrom typing import List\n\nclass TaskAnalysis(BaseModel):\n priority: str\n estimated_hours: float\n dependencies: List[str]\n risk_level: str\n\n# Using responses.parse for automatic parsing\nresponse = client.responses.parse(\n model='gpt-4.1-mini',\n input='Analyze this project task: Build user authentication system',\n text_format=TaskAnalysis,\n instructions='Analyze the given task and provide structured output.'\n)\n\n# Access parsed data directly\ntask_data = response.output[0].content[0].parsed\nprint(f'Priority: {task_data.priority}')\nprint(f'Estimated hours: {task_data.estimated_hours}')\n\n# Alternative: Manual structured output\nschema = {\n \"type\": \"json_schema\",\n \"json_schema\": {\n \"name\": \"task_analysis\",\n \"strict\": True,\n \"schema\": {\n \"type\": \"object\",\n \"properties\": {\n \"priority\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n \"estimated_hours\": {\"type\": \"number\"},\n \"dependencies\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n \"risk_level\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]}\n },\n \"required\": [\"priority\", \"estimated_hours\", \"dependencies\", \"risk_level\"],\n \"additionalProperties\": False\n }\n }\n}\n\nresponse = client.responses.create(\n model='gpt-4.1-mini',\n input='Analyze this project task: Build user authentication system',\n instructions='Analyze the given task and provide structured output.',\n text={\n \"format\": schema\n }\n)"
},
"nodejs_structured_outputs": {
"description": "Node.js structured outputs with Zod",
"code": "import { z } from 'zod';\nimport { zodResponseFormat } from 'openai/helpers/zod';\n\nconst TaskAnalysisSchema = z.object({\n priority: z.enum(['low', 'medium', 'high']),\n estimated_hours: z.number(),\n dependencies: z.array(z.string()),\n risk_level: z.enum(['low', 'medium', 'high'])\n});\n\ntype TaskAnalysis = z.infer<typeof TaskAnalysisSchema>;\n\n// Using responses.parse for automatic parsing\nconst response = await client.responses.parse({\n model: 'gpt-4.1-mini',\n input: 'Analyze this project task: Build user authentication system',\n response_format: zodResponseFormat(TaskAnalysisSchema, 'task_analysis'),\n instructions: 'Analyze the given task and provide structured output.'\n});\n\n// Access parsed data directly\nconst taskData = response.output_parsed;\nconsole.log(`Priority: ${taskData.priority}`);\nconsole.log(`Estimated hours: ${taskData.estimated_hours}`);\n\n// Alternative: Manual JSON schema\nconst manualResponse = await client.responses.create({\n model: 'gpt-4.1-mini',\n input: 'Analyze this project task: Build user authentication system',\n instructions: 'Analyze the given task and provide structured output.',\n text: {\n format: {\n type: 'json_schema',\n json_schema: {\n name: 'task_analysis',\n strict: true,\n schema: {\n type: 'object',\n properties: {\n priority: { type: 'string', enum: ['low', 'medium', 'high'] },\n estimated_hours: { type: 'number' },\n dependencies: { type: 'array', items: { type: 'string' } },\n risk_level: { type: 'string', enum: ['low', 'medium', 'high'] }\n },\n required: ['priority', 'estimated_hours', 'dependencies', 'risk_level'],\n additionalProperties: false\n }\n }\n }\n }\n});"
},
"streaming_implementation": {
"description": "Streaming responses with proper event handling",
"python_code": "import asyncio\n\n# Python streaming\nresponse_stream = client.responses.create(\n model='gpt-4.1-mini',\n input='Write a short story about AI',\n instructions='Write a creative short story.',\n stream=True\n)\n\nfor event in response_stream:\n if event.type == 'response.output_text.delta':\n print(event.delta, end='', flush=True)\n elif event.type == 'response.completed':\n print('\\n--- Response completed ---')\n usage = event.response.usage\n print(f'Total tokens: {usage.total_tokens}')\n\n# Async Python streaming\nasync def stream_response():\n response_stream = await client.responses.create(\n model='gpt-4.1-mini',\n input='Write a short story about AI',\n instructions='Write a creative short story.',\n stream=True\n )\n \n async for event in response_stream:\n if event.type == 'response.output_text.delta':\n print(event.delta, end='', flush=True)\n elif event.type == 'response.completed':\n print('\\n--- Response completed ---')\n usage = event.response.usage\n print(f'Total tokens: {usage.total_tokens}')\n\nasyncio.run(stream_response())",
"nodejs_code": "// Node.js streaming\nconst responseStream = await client.responses.create({\n model: 'gpt-4.1-mini',\n input: 'Write a short story about AI',\n instructions: 'Write a creative short story.',\n stream: true\n});\n\nfor await (const event of responseStream) {\n if (event.type === 'response.output_text.delta') {\n process.stdout.write(event.delta);\n } else if (event.type === 'response.completed') {\n console.log('\\n--- Response completed ---');\n const usage = event.response.usage;\n console.log(`Total tokens: ${usage.total_tokens}`);\n }\n}\n\n// With error handling\ntry {\n const responseStream = await client.responses.create({\n model: 'gpt-4.1-mini',\n input: 'Write a short story about AI',\n instructions: 'Write a creative short story.',\n stream: true,\n timeout: 60000\n });\n\n for await (const event of responseStream) {\n switch (event.type) {\n case 'response.output_text.delta':\n process.stdout.write(event.delta);\n break;\n case 'response.completed':\n console.log('\\n--- Completed ---');\n break;\n case 'response.failed':\n console.error('Response failed:', event.response.error);\n break;\n }\n }\n} catch (error) {\n console.error('Stream error:', error);\n}"
},
"error_handling_comprehensive": {
"description": "Comprehensive error handling with retry logic",
"python_code": "import openai\nimport asyncio\nimport random\nfrom typing import Optional, Any\n\nasync def make_openai_request_with_retry(\n prompt: str,\n model: str = 'gpt-4.1-mini',\n max_retries: int = 3,\n base_delay: float = 1.0\n) -> Optional[Any]:\n \"\"\"Make OpenAI request with exponential backoff retry\"\"\"\n \n for attempt in range(max_retries + 1):\n try:\n response = await client.responses.create(\n model=model,\n input=prompt,\n instructions='You are a helpful assistant.',\n timeout=30\n )\n \n # Log successful request\n usage = response.usage\n print(f'Success: {usage.total_tokens} tokens used')\n return response\n \n except openai.RateLimitError as e:\n if attempt < max_retries:\n delay = base_delay * (2 \*\* attempt) + random.uniform(0, 1)\n print(f'Rate limited, retrying in {delay:.2f}s...')\n await asyncio.sleep(delay)\n else:\n print('Rate limit exceeded, falling back to cache or lower model')\n # Try fallback model\n try:\n return await client.responses.create(\n model='gpt-4.1-nano',\n input=prompt,\n instructions='You are a helpful assistant.'\n )\n except Exception:\n raise e\n \n except openai.APIConnectionError as e:\n if attempt < max*retries:\n delay = base_delay * (2 ** attempt)\n print(f'Connection error, retrying in {delay:.2f}s...')\n await asyncio.sleep(delay)\n else:\n raise e\n \n except openai.APIError as e:\n if e.status_code >= 500 and attempt < max_retries:\n delay = base_delay \* (2 ** attempt)\n print(f'Server error {e.status*code}, retrying in {delay:.2f}s...')\n await asyncio.sleep(delay)\n else:\n raise e\n \n except Exception as e:\n print(f'Unexpected error: {e}')\n raise e\n \n return None\n\n# Usage\nresponse = await make_openai_request_with_retry(\n 'Explain quantum computing in simple terms'\n)",
"nodejs_code": "import OpenAI from 'openai';\n\ninterface RetryOptions {\n maxRetries?: number;\n baseDelay?: number;\n maxDelay?: number;\n}\n\nasync function makeOpenAIRequestWithRetry(\n prompt: string,\n model: string = 'gpt-4.1-mini',\n options: RetryOptions = {}\n): Promise<OpenAI.Response | null> {\n const {\n maxRetries = 3,\n baseDelay = 1000,\n maxDelay = 60000\n } = options;\n\n for (let attempt = 0; attempt <= maxRetries; attempt++) {\n try {\n const response = await client.responses.create({\n model,\n input: prompt,\n instructions: 'You are a helpful assistant.',\n timeout: 30000\n });\n\n // Log successful request\n const usage = response.usage;\n console.log(`Success: ${usage.total_tokens} tokens used`);\n return response;\n\n } catch (error: any) {\n if (error instanceof OpenAI.RateLimitError) {\n if (attempt < maxRetries) {\n const delay = Math.min(\n baseDelay * Math.pow(2, attempt) + Math.random() _ 1000,\n maxDelay\n );\n console.log(`Rate limited, retrying in ${delay}ms...`);\n await new Promise(resolve => setTimeout(resolve, delay));\n } else {\n console.log('Rate limit exceeded, trying fallback model');\n try {\n return await client.responses.create({\n model: 'gpt-4.1-nano',\n input: prompt,\n instructions: 'You are a helpful assistant.'\n });\n } catch (fallbackError) {\n throw error;\n }\n }\n } else if (error instanceof OpenAI.APIConnectionError) {\n if (attempt < maxRetries) {\n const delay = Math.min(baseDelay _ Math.pow(2, attempt), maxDelay);\n console.log(`Connection error, retrying in ${delay}ms...`);\n await new Promise(resolve => setTimeout(resolve, delay));\n } else {\n throw error;\n }\n } else if (error instanceof OpenAI.APIError) {\n if (error.status >= 500 && attempt < maxRetries) {\n const delay = Math.min(baseDelay _ Math.pow(2, attempt), maxDelay);\n console.log(`Server error ${error.status}, retrying in ${delay}ms...`);\n await new Promise(resolve => setTimeout(resolve, delay));\n } else {\n throw error;\n }\n } else {\n console.error('Unexpected error:', error);\n throw error;\n }\n }\n }\n }\n\n return null;\n}\n\n// Usage\nconst response = await makeOpenAIRequestWithRetry(\n 'Explain quantum computing in simple terms'\n);"
},
"conversation_state_management": {
"description": "Multi-turn conversation with state management",
"code": "// Node.js conversation state example\ninterface ConversationState {\n responseIds: string[];\n context: string;\n userId: string;\n}\n\nclass ConversationManager {\n private conversations = new Map<string, ConversationState>();\n\n async sendMessage(\n userId: string,\n message: string,\n instructions?: string\n ): Promise<OpenAI.Response> {\n const state = this.conversations.get(userId) || {\n responseIds: [],\n context: '',\n userId\n };\n\n const response = await client.responses.create({\n model: 'gpt-4.1-mini',\n input: message,\n instructions: instructions || 'You are a helpful assistant. Maintain conversation context.',\n previous_response_id: state.responseIds[state.responseIds.length - 1],\n metadata: {\n user_id: userId,\n conversation_turn: state.responseIds.length.toString()\n }\n });\n\n // Update conversation state\n state.responseIds.push(response.id);\n state.context += `\\nUser: ${message}\\nAssistant: ${response.output_text}`;\n this.conversations.set(userId, state);\n\n return response;\n }\n\n async getConversationHistory(userId: string): Promise<string[]> {\n const state = this.conversations.get(userId);\n if (!state) return [];\n\n // Retrieve all responses in the conversation\n const responses = await Promise.all(\n state.responseIds.map(id => client.responses.retrieve(id))\n );\n\n return responses.map(r => r.output_text);\n }\n\n clearConversation(userId: string): void {\n this.conversations.delete(userId);\n }\n}\n\n// Usage\nconst conversationManager = new ConversationManager();\n\nconst response1 = await conversationManager.sendMessage(\n 'user123',\n 'Hello, I need help with Python programming'\n);\n\nconst response2 = await conversationManager.sendMessage(\n 'user123',\n 'Specifically, how do I handle exceptions?'\n); // This will have context from the previous message"
},
"function_calling_tools": {
"description": "Function calling with tools",
"python_code": "from typing import Dict, Any\nimport json\n\n# Define tool functions\ndef get_weather(location: str, unit: str = 'celsius') -> Dict[str, Any]:\n \"\"\"Get current weather for a location\"\"\"\n # Simulate weather API call\n return {\n 'location': location,\n 'temperature': 22 if unit == 'celsius' else 72,\n 'unit': unit,\n 'conditions': 'sunny'\n }\n\ndef calculate(expression: str) -> float:\n \"\"\"Safely evaluate mathematical expressions\"\"\"\n try:\n # In production, use a proper math parser\n result = eval(expression)\n return float(result)\n except Exception as e:\n raise ValueError(f'Invalid expression: {e}')\n\n# Tool definitions\ntools = [\n {\n 'type': 'function',\n 'function': {\n 'name': 'get_weather',\n 'description': 'Get current weather information for a specific location',\n 'parameters': {\n 'type': 'object',\n 'properties': {\n 'location': {\n 'type': 'string',\n 'description': 'The city and country, e.g. San Francisco, CA'\n },\n 'unit': {\n 'type': 'string',\n 'enum': ['celsius', 'fahrenheit'],\n 'description': 'Temperature unit'\n }\n },\n 'required': ['location']\n }\n }\n },\n {\n 'type': 'function',\n 'function': {\n 'name': 'calculate',\n 'description': 'Perform mathematical calculations',\n 'parameters': {\n 'type': 'object',\n 'properties': {\n 'expression': {\n 'type': 'string',\n 'description': 'Mathematical expression to evaluate'\n }\n },\n 'required': ['expression']\n }\n }\n }\n]\n\n# Function calling response\nresponse = client.responses.create(\n model='gpt-4.1-mini',\n input='What is the weather in Tokyo and calculate 15 _ 24?',\n instructions='You have access to weather and calculator tools. Use them to answer the user\\'s questions.',\n tools=tools,\n tool*choice='auto'\n)\n\n# Process function calls\nfor output_item in response.output:\n if hasattr(output_item, 'type'):\n if output_item.type == 'function_call':\n function_name = output_item.name\n arguments = json.loads(output_item.arguments)\n \n # Execute the function\n if function_name == 'get_weather':\n result = get_weather(**arguments)\n elif function_name == 'calculate':\n result = calculate(**arguments)\n else:\n result = {'error': f'Unknown function: {function_name}'}\n \n print(f'Function {function_name} called with {arguments}')\n print(f'Result: {result}')\n elif output_item.type == 'message':\n for content in output_item.content:\n if content.type == 'output_text':\n print(f'Assistant: {content.text}')",
"nodejs_code": "// TypeScript function calling\ninterface WeatherResult {\n location: string;\n temperature: number;\n unit: string;\n conditions: string;\n}\n\ninterface CalculationResult {\n result: number;\n expression: string;\n}\n\n// Tool functions\nfunction getWeather(location: string, unit: string = 'celsius'): WeatherResult {\n // Simulate weather API call\n return {\n location,\n temperature: unit === 'celsius' ? 22 : 72,\n unit,\n conditions: 'sunny'\n };\n}\n\nfunction calculate(expression: string): CalculationResult {\n try {\n // In production, use a proper math parser\n const result = eval(expression);\n return { result: Number(result), expression };\n } catch (error) {\n throw new Error(`Invalid expression: ${error}`);\n }\n}\n\n// Tool definitions\nconst tools = [\n {\n type: 'function' as const,\n function: {\n name: 'get_weather',\n description: 'Get current weather information for a specific location',\n parameters: {\n type: 'object',\n properties: {\n location: {\n type: 'string',\n description: 'The city and country, e.g. San Francisco, CA'\n },\n unit: {\n type: 'string',\n enum: ['celsius', 'fahrenheit'],\n description: 'Temperature unit'\n }\n },\n required: ['location']\n }\n }\n },\n {\n type: 'function' as const,\n function: {\n name: 'calculate',\n description: 'Perform mathematical calculations',\n parameters: {\n type: 'object',\n properties: {\n expression: {\n type: 'string',\n description: 'Mathematical expression to evaluate'\n }\n },\n required: ['expression']\n }\n }\n }\n];\n\n// Function calling response\nconst response = await client.responses.create({\n model: 'gpt-4.1-mini',\n input: 'What is the weather in Tokyo and calculate 15 * 24?',\n instructions: 'You have access to weather and calculator tools. Use them to answer the user\\'s questions.',\n tools,\n tool_choice: 'auto'\n});\n\n// Process function calls\nfor (const outputItem of response.output) {\n if ('type' in outputItem) {\n if (outputItem.type === 'function_call') {\n const functionName = outputItem.name;\n const args = JSON.parse(outputItem.arguments);\n \n let result: any;\n try {\n switch (functionName) {\n case 'get_weather':\n result = getWeather(args.location, args.unit);\n break;\n case 'calculate':\n result = calculate(args.expression);\n break;\n default:\n result = { error: `Unknown function: ${functionName}` };\n }\n \n console.log(`Function ${functionName} called with:`, args);\n console.log('Result:', result);\n } catch (error) {\n console.error(`Error calling ${functionName}:`, error);\n }\n } else if (outputItem.type === 'message') {\n for (const content of outputItem.content) {\n if (content.type === 'output_text') {\n console.log(`Assistant: ${content.text}`);\n }\n }\n }\n }\n}"
}
}
},
"common_mistakes_prevention": {
"depth": 1,
"trigger": "mistake-prevention",
"critical_errors_to_avoid": [
{
"mistake": "Using chat.completions.create() instead of responses.create()",
"wrong": "const response = await client.chat.completions.create({model: 'gpt-4.1', messages: [{role: 'user', content: 'Hello'}]});",
"correct": "const response = await client.responses.create({model: 'gpt-4.1', input: 'Hello', instructions: 'You are helpful.'});",
"explanation": "The Responses API is newer and supports better features like conversation state management"
},
{
"mistake": "Hard-coding API keys",
"wrong": "const client = new OpenAI({apiKey: 'sk-abc123def456'});",
"correct": "const client = new OpenAI({apiKey: process.env.OPENAI_API_KEY});",
"explanation": "Never hard-code API keys. Always use environment variables"
},
{
"mistake": "Not handling rate limits",
"wrong": "const response = await client.responses.create({...});",
"correct": "const response = await makeOpenAIRequestWithRetry(prompt); // Use retry wrapper",
"explanation": "Always implement exponential backoff for rate limit handling"
},
{
"mistake": "Ignoring token usage",
"wrong": "const response = await client.responses.create({...}); return response.output_text;",
"correct": "const response = await client.responses.create({...}); logger.info(`Tokens used: ${response.usage.total_tokens}`); return response.output_text;",
"explanation": "Always log token usage for cost tracking"
},
{
"mistake": "Not using structured outputs when possible",
"wrong": "const response = await client.responses.create({input: 'Extract name and email from: John Doe john@example.com'});",
"correct": "const schema = z.object({name: z.string(), email: z.string()}); const response = await client.responses.parse({input: '...', response_format: zodResponseFormat(schema, 'contact')});",
"explanation": "Use structured outputs with strict:true for reliable data extraction"
},
{
"mistake": "Creating new client instances repeatedly",
"wrong": "function callOpenAI(prompt) { const client = new OpenAI(); return client.responses.create({...}); }",
"correct": "const client = new OpenAI(); // Reuse this instance everywhere",
"explanation": "Reuse client instances for better performance and connection pooling"
},
{
"mistake": "Not setting timeouts",
"wrong": "const response = await client.responses.create({model: 'gpt-4.1', input: 'Hello'});",
"correct": "const response = await client.responses.create({model: 'gpt-4.1', input: 'Hello', timeout: 30000});",
"explanation": "Always set timeouts to prevent hanging requests"
},
{
"mistake": "Using deprecated models",
"wrong": "const response = await client.responses.create({model: 'gpt-3.5-turbo', input: 'Hello'});",
"correct": "const response = await client.responses.create({model: 'gpt-4.1-mini', input: 'Hello'});",
"explanation": "Use gpt-4.1 family models (nano/mini/base) for best performance"
}
]
},
"quick_reference_patterns": {
"depth": 1,
"basic_patterns": {
"simple_request": "client.responses.create({model: 'gpt-4.1-mini', input: 'Hello', instructions: 'Be helpful'})",
"structured_output": "client.responses.parse({model: 'gpt-4.1-mini', input: 'Extract data', text_format: MyPydanticModel})",
"streaming": "for await (const event of client.responses.create({model: 'gpt-4.1-mini', input: 'Hello', stream: true}))",
"with_tools": "client.responses.create({model: 'gpt-4.1-mini', input: 'Hello', tools: [weatherTool], tool_choice: 'auto'})",
"conversation": "client.responses.create({model: 'gpt-4.1-mini', input: 'Follow up', previous_response_id: lastResponse.id})",
"background_task": "const resp = await client.responses.create({background: true, ...}); while(resp.status === 'in_progress') await checkStatus(resp.id);"
}
},
"sections": {
"core_implementation_rules": {
"depth": 1,
"trigger": "openai-sdk-usage",
"rules": [
{"name": "mandatory_responses_api", "when": "writing new OpenAI integration code", "to": "use modern API", "you_must": "use client.responses.create() NOT client.chat.completions.create()", "validation_criteria": ["responses.create called", "No chat.completions in new code"], "compliant_example": "client.responses.create({model:'gpt-4.1-mini',input:'Hello',instructions:'Be helpful'})", "non_compliant_example": "client.chat.completions.create({model:'gpt-4.1',messages:[{role:'user',content:'Hello'}]})", "dependencies": []},
{"name": "environment_based_auth", "when": "initializing OpenAI client", "to": "secure credentials", "you_must": "use environment variables for API keys", "validation_criteria": ["process.env.OPENAI_API_KEY or os.getenv used", "No hard-coded keys"], "compliant_example": "new OpenAI({apiKey: process.env.OPENAI_API_KEY})", "non_compliant_example": "new OpenAI({apiKey: 'sk-abc123'})", "dependencies": []},
{"name": "model_preference_hierarchy", "when": "selecting models", "to": "balance cost and performance", "you_must": "use gpt-4.1-nano for prototyping, gpt-4.1-mini for most tasks, gpt-4.1 for complex tasks", "validation_criteria": ["Model selection follows nano→mini→base pattern"], "compliant_example": "model: config.phase === 'prototype' ? 'gpt-4.1-nano' : 'gpt-4.1-mini'", "non_compliant_example": "model: 'gpt-3.5-turbo'", "dependencies": []},
{"name": "client_instance_reuse", "when": "making multiple API calls", "to": "optimize performance", "you_must": "reuse single client instance across application", "validation_criteria": ["Client instantiated once", "Client reused for all calls"], "compliant_example": "const client = new OpenAI({...}); // reuse everywhere", "non_compliant_example": "function callAPI() { const client = new OpenAI(); ... }", "dependencies": ["environment_based_auth"]},
{"name": "comprehensive_error_handling", "when": "making API requests", "to": "handle failures gracefully", "you_must": "wrap API calls with try-catch and specific error handling", "validation_criteria": ["RateLimitError handled", "APIConnectionError handled", "Exponential backoff implemented"], "compliant_example": "try { return await client.responses.create({...}); } catch(error) { if(error instanceof OpenAI.RateLimitError) { await backoff(); retry(); } }", "non_compliant_example": "const response = await client.responses.create({...}); // no error handling", "dependencies": []},
{"name": "token_usage_logging", "when": "processing responses", "to": "track costs", "you_must": "log response.usage for every API call", "validation_criteria": ["usage.total_tokens logged", "Cost calculation present"], "compliant_example": "const resp = await client.responses.create({...}); logger.info(`Tokens: ${resp.usage.total_tokens}`);", "non_compliant_example": "const resp = await client.responses.create({...}); return resp.output_text;", "dependencies": []},
{"name": "structured_outputs_enforcement", "when": "extracting data or structured information", "to": "ensure reliability", "you_must": "use responses.parse() with schemas or strict JSON schema", "validation_criteria": ["responses.parse used OR strict:true schema", "Pydantic/Zod schema defined"], "compliant_example": "client.responses.parse({..., text_format: MySchema})", "non_compliant_example": "client.responses.create({input: 'extract data'}); // no schema", "dependencies": []},
{"name": "timeout_configuration", "when": "making API requests", "to": "prevent hanging", "you_must": "set appropriate timeout values", "validation_criteria": ["timeout parameter present", "Different timeouts for stream vs non-stream"], "compliant_example": "client.responses.create({timeout: 30000, ...})", "non_compliant_example": "client.responses.create({...}); // no timeout", "dependencies": []}
]
}
}
}
