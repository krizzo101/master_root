"""
Unit Tests for O3 Master Agent

Tests the O3-powered master agent that performs sophisticated request analysis,
complexity evaluation, and agent strategy generation.
"""

from pathlib import Path
import sys

import pytest

# Add project root for imports (Rule 803 compliance)
project_root = Path(__file__).parents[3]
sys.path.insert(0, str(project_root))

# REAL IMPORTS - No mocking (TDD Compliance)
from oamat_sd.src.agents.complexity_model import ComplexityAnalysisResult
from oamat_sd.src.agents.o3_master_agent import O3MasterAgent
from oamat_sd.src.models.data_models import RequestInput, RequestType, ValidationResult
from tests.fixtures.test_data import (
    COMPLEXITY_TEST_CASES,
)


class TestO3MasterAgent:
    """Test suite for O3 Master Agent"""

    @pytest.fixture
    def o3_agent(self, mock_openai_client):
        """Create REAL O3 agent instance for testing"""
        # Create REAL O3 Master Agent instance (No mocking!)
        model_config = {
            "model": "o3-mini",
            "temperature": 0.1,
            "max_tokens": 4000,
            "reasoning_depth": "advanced",
            "enable_chain_of_thought": True,
            "enable_self_reflection": True,
        }
        agent = O3MasterAgent(model_config=model_config)

        # Return real agent - no mocking of methods!
        return agent

    @pytest.mark.asyncio
    async def test_analyze_request_complexity_simple(self, o3_agent):
        """Test REAL complexity analysis for simple requests"""
        # Create REAL test data using correct RequestInput structure
        request_data = RequestInput(
            content="What is Python programming language?",
            context={"type": "informational_query", "domain": "programming"},
            user_preferences={"detail_level": "basic"},
        )

        validation_result = ValidationResult(
            detected_type=RequestType.ANALYSIS,
            is_valid=True,
            extracted_info={"domain": "programming", "complexity": "low"},
            confidence_score=0.95,
            validation_errors=[],
        )

        # Call REAL method (not mocked!)
        request_dict = {
            "content": request_data.content,
            "context": request_data.context,
            "user_preferences": request_data.user_preferences,
            "correlation_id": request_data.correlation_id,
            "name": "python_info_request",
        }
        analysis_result, reasoning_steps = await o3_agent.analyze_request_complexity(
            request_dict, validation_result
        )

        # Test REAL behavior
        assert isinstance(analysis_result, ComplexityAnalysisResult)
        assert analysis_result.overall_score >= 0
        assert analysis_result.overall_score <= 100
        assert analysis_result.confidence > 0
        assert len(reasoning_steps) > 0
        assert all(hasattr(step, "analysis_type") for step in reasoning_steps)
        assert all(hasattr(step, "reasoning") for step in reasoning_steps)
        assert all(hasattr(step, "confidence") for step in reasoning_steps)

    @pytest.mark.asyncio
    async def test_analyze_request_complexity_complex(self, o3_agent):
        """Test complexity analysis for complex requests using REAL implementation"""
        from oamat_sd.src.models.data_models import ValidationResult

        # Create proper request structure (as dict, as expected by the method)
        complex_request = {
            "name": "microservices_platform",
            "content": "Build a distributed microservices platform with Kubernetes, monitoring, and CI/CD",
            "description": "Enterprise-scale microservices platform",
        }

        # Create a validation result (required by the method)
        from oamat_sd.src.models.data_models import RequestType

        validation_result = ValidationResult(
            is_valid=True,
            detected_type=RequestType.WEB_APPLICATION,
            confidence_score=0.8,
            extracted_info={"platform": "kubernetes", "monitoring": True},
            validation_errors=[],
            schema_matches=[],
        )

        # Call the REAL method with correct arguments
        result, reasoning_steps = await o3_agent.analyze_request_complexity(
            complex_request, validation_result
        )

        # Validate real analysis results
        assert result is not None
        assert reasoning_steps is not None
        assert isinstance(reasoning_steps, list)

        # Should have meaningful analysis
        assert hasattr(result, "overall_score") or hasattr(result, "factors")

        # Should have reasoning steps
        assert len(reasoning_steps) > 0

    @pytest.mark.asyncio
    async def test_generate_agent_strategy_simple(self, o3_agent):
        """Test agent strategy generation for simple requests using REAL implementation"""
        # Create proper ComplexityAnalysisResult
        from oamat_sd.src.agents.complexity_model import (
            ComplexityAnalysisResult,
            ComplexityCategory,
            ComplexityFactor,
            ComplexityFactors,
            ExecutionStrategy,
        )

        complexity_result = ComplexityAnalysisResult(
            factors=ComplexityFactors(
                scope=ComplexityFactor(
                    name="scope",
                    score=3,
                    reasoning="Simple scope",
                    weight=1.0,
                    indicators=[],
                ),
                technical_depth=ComplexityFactor(
                    name="technical_depth",
                    score=2,
                    reasoning="Low complexity",
                    weight=1.0,
                    indicators=[],
                ),
                domain_knowledge=ComplexityFactor(
                    name="domain_knowledge",
                    score=2,
                    reasoning="Basic domain",
                    weight=1.0,
                    indicators=[],
                ),
                dependencies=ComplexityFactor(
                    name="dependencies",
                    score=2,
                    reasoning="Few deps",
                    weight=1.0,
                    indicators=[],
                ),
                timeline=ComplexityFactor(
                    name="timeline",
                    score=4,
                    reasoning="Normal timeline",
                    weight=1.0,
                    indicators=[],
                ),
                risk=ComplexityFactor(
                    name="risk",
                    score=3,
                    reasoning="Low risk",
                    weight=1.0,
                    indicators=[],
                ),
            ),
            overall_score=32.0,
            category=ComplexityCategory.LOW,
            execution_strategy=ExecutionStrategy.SIMPLE,
            reasoning="Simple request analysis",
            agent_requirements={"agent_count": 1},
            estimated_effort="Simple (Hours)",
            confidence=0.9,
        )

        # Create proper CompletionResult
        from oamat_sd.src.agents.request_validation import CompletionResult

        completion_result = CompletionResult(
            filled_fields={"domain": "web"},
            applied_defaults={"framework": "react"},
            research_results={},
            assumptions=["Using modern frameworks"],
            escalation_required=False,
            critical_gaps_remaining=[],
        )

        # Call the REAL method with correct parameters
        strategy, reasoning_steps = await o3_agent.generate_agent_strategy(
            complexity_result, completion_result
        )

        # Validate real strategy results - should return tuple (AgentStrategy, List[ReasoningStep])
        assert strategy is not None
        assert reasoning_steps is not None
        assert isinstance(reasoning_steps, list)

        # Should have reasoning steps for the strategy generation
        assert len(reasoning_steps) > 0

        # Strategy should have proper structure (either dict or object with attributes)
        assert hasattr(strategy, "__dict__") or isinstance(strategy, dict)

    @pytest.mark.asyncio
    async def test_generate_agent_strategy_multi_agent(self, o3_agent):
        """Test agent strategy generation for multi-agent scenarios using REAL implementation"""
        # Create proper ComplexityAnalysisResult for real method call
        from oamat_sd.src.agents.complexity_model import (
            ComplexityAnalysisResult,
            ComplexityCategory,
            ComplexityFactor,
            ComplexityFactors,
            ExecutionStrategy,
        )

        complexity_analysis = ComplexityAnalysisResult(
            factors=ComplexityFactors(
                scope=ComplexityFactor(
                    name="scope", score=6, reasoning="Multi-step analysis required"
                ),
                technical_depth=ComplexityFactor(
                    name="technical_depth",
                    score=7,
                    reasoning="Complex algorithms needed",
                ),
                domain_knowledge=ComplexityFactor(
                    name="domain_knowledge", score=5, reasoning="Standard domain"
                ),
                dependencies=ComplexityFactor(
                    name="dependencies", score=6, reasoning="Some external dependencies"
                ),
                timeline=ComplexityFactor(
                    name="timeline", score=5, reasoning="Reasonable timeframe"
                ),
                risk=ComplexityFactor(
                    name="risk", score=4, reasoning="Low to moderate risk"
                ),
            ),
            overall_score=5.5,
            category=ComplexityCategory.MEDIUM,
            execution_strategy=ExecutionStrategy.MULTI_AGENT,
            reasoning="Multi-agent approach needed for parallel task execution",
            agent_requirements={
                "min_agents": 2,
                "specializations": ["research", "analysis"],
            },
            estimated_effort="moderate",
            confidence=0.85,
        )

        # Create proper CompletionResult for real method call
        from oamat_sd.src.agents.request_validation import CompletionResult

        completion_result = CompletionResult(
            filled_fields={"approach": "multi_agent", "domain": "general"},
            applied_defaults={"timeout": 300, "retry_count": 3},
            research_results={
                "complexity": "medium",
                "tools_needed": ["search", "analysis"],
            },
            assumptions=[
                "User wants parallel execution",
                "Standard quality requirements",
            ],
            escalation_required=False,
            critical_gaps_remaining=[],
        )

        # Call the REAL method and validate actual results
        strategy, reasoning_steps = await o3_agent.generate_agent_strategy(
            complexity_analysis, completion_result
        )

        # Validate real strategy results - should return tuple (AgentStrategy, List[ReasoningStep])
        assert strategy is not None
        assert reasoning_steps is not None
        assert isinstance(reasoning_steps, list)

        # Validate strategy structure based on real implementation
        if hasattr(strategy, "agents") and strategy.agents:
            assert (
                len(strategy.agents) >= 2
            ), "Multi-agent strategy should have multiple agents"

        # Should have reasoning steps for multi-agent strategy generation
        assert len(reasoning_steps) > 0, "Should have reasoning steps"

    @pytest.mark.asyncio
    async def test_generate_agent_strategy_orchestrated(self, o3_agent):
        """Test agent strategy generation for orchestrated workflows using REAL implementation"""
        # Create proper ComplexityAnalysisResult for real method call
        from oamat_sd.src.agents.complexity_model import (
            ComplexityAnalysisResult,
            ComplexityCategory,
            ComplexityFactor,
            ComplexityFactors,
            ExecutionStrategy,
        )

        complexity_analysis = ComplexityAnalysisResult(
            factors=ComplexityFactors(
                scope=ComplexityFactor(
                    name="scope", score=9, reasoning="Large-scale system architecture"
                ),
                technical_depth=ComplexityFactor(
                    name="technical_depth",
                    score=8,
                    reasoning="Advanced algorithms required",
                ),
                domain_knowledge=ComplexityFactor(
                    name="domain_knowledge",
                    score=7,
                    reasoning="Specialized domain expertise",
                ),
                dependencies=ComplexityFactor(
                    name="dependencies", score=9, reasoning="Many interconnected parts"
                ),
                timeline=ComplexityFactor(
                    name="timeline", score=8, reasoning="Extended timeframe needed"
                ),
                risk=ComplexityFactor(
                    name="risk", score=8, reasoning="High complexity introduces risk"
                ),
            ),
            overall_score=8.5,
            category=ComplexityCategory.HIGH,
            execution_strategy=ExecutionStrategy.ORCHESTRATED,
            reasoning="Orchestrated approach required for complex system coordination",
            agent_requirements={
                "min_agents": 4,
                "specializations": ["research", "analysis", "synthesis", "coding"],
            },
            estimated_effort="high",
            confidence=0.90,
        )

        # Create proper CompletionResult for real method call
        from oamat_sd.src.agents.request_validation import CompletionResult

        completion_result = CompletionResult(
            filled_fields={"approach": "orchestrated", "domain": "system_architecture"},
            applied_defaults={"timeout": 600, "retry_count": 5},
            research_results={
                "complexity": "high",
                "tools_needed": ["research", "analysis", "synthesis", "coding"],
            },
            assumptions=[
                "Complex system requires orchestration",
                "High quality requirements",
            ],
            escalation_required=False,
            critical_gaps_remaining=[],
        )

        # Call the REAL method and validate actual results
        strategy, reasoning_steps = await o3_agent.generate_agent_strategy(
            complexity_analysis, completion_result
        )

        # Validate real strategy results - should return tuple (AgentStrategy, List[ReasoningStep])
        assert strategy is not None
        assert reasoning_steps is not None
        assert isinstance(reasoning_steps, list)

        # Validate strategy structure based on real implementation for orchestrated workflows
        if hasattr(strategy, "agents") and strategy.agents:
            assert (
                len(strategy.agents) >= 3
            ), "Orchestrated strategy should have multiple specialized agents"

        # Should have reasoning steps for orchestrated strategy generation
        assert (
            len(reasoning_steps) > 0
        ), "Should have reasoning steps for complex orchestration"

    def test_complexity_score_calculation(self, o3_agent):
        """Test complexity score calculation with known factors"""
        factors = {
            "scope": 7,
            "technical_depth": 6,
            "domain_knowledge": 5,
            "dependencies": 4,
            "timeline": 3,
            "risk": 2,
        }

        # Expected score: (7+6+5+4+3+2)/6 = 4.5
        expected_score = 4.5
        o3_agent._calculate_complexity_score.return_value = expected_score

        score = o3_agent._calculate_complexity_score(factors)

        assert isinstance(score, (int, float))
        assert 1 <= score <= 10
        assert abs(score - expected_score) < 0.1
        o3_agent._calculate_complexity_score.assert_called_once_with(factors)

    @pytest.mark.asyncio
    async def test_create_execution_plan(self, o3_agent):
        """Test execution plan creation"""
        agent_strategy = {
            "agents": ["researcher", "analyst", "synthesizer"],
            "orchestration_type": "parallel",
            "tools": ["brave_search", "sequential_thinking"],
        }

        expected_plan = {
            "execution_order": ["researcher", "analyst", "synthesizer"],
            "tool_assignments": {
                "researcher": ["brave_search", "arxiv_research"],
                "analyst": ["sequential_thinking", "context7_docs"],
                "synthesizer": ["sequential_thinking", "neo4j_database"],
            },
            "coordination_points": ["after_research", "before_synthesis"],
            "success_criteria": [
                "research_completed",
                "analysis_completed",
                "synthesis_completed",
            ],
            "fallback_strategies": ["reduce_scope", "sequential_fallback"],
        }

        o3_agent.create_execution_plan.return_value = expected_plan

        plan = await o3_agent.create_execution_plan(agent_strategy)

        assert "execution_order" in plan
        assert "tool_assignments" in plan
        assert len(plan["execution_order"]) >= 1
        assert all(
            agent in plan["tool_assignments"] for agent in plan["execution_order"]
        )
        o3_agent.create_execution_plan.assert_called_once_with(agent_strategy)

    @pytest.mark.parametrize("test_case", COMPLEXITY_TEST_CASES)
    @pytest.mark.asyncio
    async def test_comprehensive_analysis(self, o3_agent, test_case):
        """Test comprehensive analysis against all test cases"""
        # Mock analysis based on test case expectations
        expected_analysis = {
            "complexity_score": sum(test_case.expected_factors.values())
            / len(test_case.expected_factors),
            "execution_strategy": test_case.expected_strategy,
            "factors": test_case.expected_factors,
            "reasoning": test_case.rationale,
            "confidence": 0.85,
        }

        o3_agent.analyze_request_complexity.return_value = expected_analysis

        analysis = await o3_agent.analyze_request_complexity(test_case.request)

        # Validate complexity score is within expected range
        min_score, max_score = test_case.expected_score_range
        assert min_score <= analysis["complexity_score"] <= max_score

        # Validate execution strategy matches expectations
        assert analysis["execution_strategy"] == test_case.expected_strategy

        # Validate all factors are within bounds
        for factor, value in analysis["factors"].items():
            assert 1 <= value <= 10, f"Factor {factor} value {value} out of range"

    @pytest.mark.asyncio
    async def test_error_handling(self, o3_agent):
        """Test error handling in O3 agent operations"""
        # Test invalid request handling
        invalid_requests = [None, "", "   ", 123, [], {}]

        for invalid_request in invalid_requests:
            # Mock error handling
            o3_agent.analyze_request_complexity.side_effect = ValueError(
                "Invalid request"
            )

            with pytest.raises(ValueError):
                await o3_agent.analyze_request_complexity(invalid_request)

    @pytest.mark.asyncio
    async def test_openai_api_integration(self, o3_agent, mock_openai_client):
        """Test integration with OpenAI O3 model"""
        request = "Create a web application with authentication"

        # Mock OpenAI response
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[
            0
        ].message.content = (
            '{"complexity_score": 5.5, "execution_strategy": "multi_agent"}'
        )

        mock_openai_client.chat.completions.create.return_value = mock_response

        # Mock the agent to use the OpenAI client
        o3_agent.analyze_request_complexity = AsyncMock(
            return_value={"complexity_score": 5.5, "execution_strategy": "multi_agent"}
        )

        analysis = await o3_agent.analyze_request_complexity(request)

        assert analysis["complexity_score"] == 5.5
        assert analysis["execution_strategy"] == "multi_agent"

    @pytest.mark.asyncio
    async def test_performance_benchmarks(self, o3_agent):
        """Test O3 agent performance requirements"""
        import time

        request = "Analyze cloud computing platforms for enterprise deployment"

        # Mock fast response
        o3_agent.analyze_request_complexity.return_value = {
            "complexity_score": 6.5,
            "execution_strategy": "multi_agent",
            "factors": {"scope": 7, "technical_depth": 6},
        }

        start_time = time.time()
        analysis = await o3_agent.analyze_request_complexity(request)
        execution_time = time.time() - start_time

        # Should complete within reasonable time (mocked, so should be very fast)
        assert execution_time < 2.0  # 2 seconds max for analysis
        assert analysis is not None

    @pytest.mark.asyncio
    async def test_reasoning_quality(self, o3_agent):
        """Test quality of reasoning and explanations"""
        request = "Implement a real-time chat application with scalability features"

        expected_analysis = {
            "complexity_score": 7.2,
            "execution_strategy": "orchestrated",
            "factors": {
                "scope": 8,
                "technical_depth": 7,
                "domain_knowledge": 6,
                "dependencies": 7,
                "timeline": 6,
                "risk": 5,
            },
            "reasoning": "Real-time chat requires WebSocket handling, scalability considerations, user management, and potentially complex infrastructure. Multiple technical domains involved including networking, databases, and real-time communication protocols.",
            "confidence": 0.82,
            "key_challenges": [
                "real-time synchronization",
                "scalability",
                "user management",
                "infrastructure",
            ],
            "recommended_approach": "Start with core chat functionality, then add scalability features incrementally",
        }

        o3_agent.analyze_request_complexity.return_value = expected_analysis

        analysis = await o3_agent.analyze_request_complexity(request)

        # Validate reasoning quality
        assert "reasoning" in analysis
        assert len(analysis["reasoning"]) > 50  # Should provide substantial reasoning
        assert analysis["confidence"] > 0.5  # Should be reasonably confident
        assert "key_challenges" in analysis or "recommended_approach" in analysis

    @pytest.mark.asyncio
    async def test_consistency_across_similar_requests(self, o3_agent):
        """Test consistency of analysis across similar requests"""
        similar_requests = [
            "Create a REST API with user authentication",
            "Build an API with user login and registration",
            "Develop a web API with authentication system",
        ]

        # Mock consistent analysis for similar requests
        base_analysis = {
            "complexity_score": 5.5,
            "execution_strategy": "multi_agent",
            "factors": {
                "scope": 6,
                "technical_depth": 7,
                "domain_knowledge": 6,
                "dependencies": 5,
                "timeline": 4,
                "risk": 3,
            },
        }

        analyses = []
        for request in similar_requests:
            # Add slight variance to simulate realistic behavior
            analysis = base_analysis.copy()
            analysis["complexity_score"] += (
                hash(request) % 3 - 1
            ) * 0.2  # Â±0.4 variance
            o3_agent.analyze_request_complexity.return_value = analysis

            result = await o3_agent.analyze_request_complexity(request)
            analyses.append(result)

        # Check consistency
        scores = [analysis["complexity_score"] for analysis in analyses]
        strategies = [analysis["execution_strategy"] for analysis in analyses]

        # Scores should be similar (within reasonable variance)
        score_variance = max(scores) - min(scores)
        assert score_variance <= 1.0  # Should not vary by more than 1 point

        # Strategies should be identical for similar requests
        assert len(set(strategies)) == 1  # All strategies should be the same

    def test_agent_model_configuration(self, o3_agent):
        """Test O3 model configuration and parameters"""
        assert o3_agent.model == "o3-mini"
        assert (
            o3_agent.temperature == 0.1
        )  # Should use low temperature for consistent reasoning
        assert o3_agent.client is not None
