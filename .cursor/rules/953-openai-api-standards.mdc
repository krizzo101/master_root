---
description: "use when INTEGRATING OPENAI APIs to ENFORCE USAGE PATTERNS for FUNCTION CALLING AND SECURITY"
globs:
alwaysApply: false
---
# OpenAI Integration Rules (2025)

## MANDATORY PRE-RESPONSE REQUIREMENTS

**BEFORE ANY OPENAI API INTEGRATION**:

### 1. **TIME AND DATE CHECK**
```python
# MANDATORY: Check current time/date at response start
from mcp_time_current_time import current_time
current_time = current_time(format="YYYY-MM-DD HH:mm:ss")
```

### 2. **ONLINE RESEARCH REQUIREMENT**
- **RESEARCH CURRENT API VERSIONS**: OpenAI API changes frequently (2025 updates)
- **VERIFY MODEL AVAILABILITY**: Confirm model names and capabilities
- **CHECK SYNTAX UPDATES**: API syntax evolves rapidly (Responses API vs Chat Completions)
- **VALIDATE PRICING**: Confirm current token costs and limits
- **PROMPTING BEST PRACTICES**: Research latest model-specific optimization techniques

### 3. **EVIDENCE-BASED API SELECTION**
- Base model choices on CURRENT documentation
- Verify feature availability through research
- No assumptions about API capabilities
- Test model-specific prompting strategies

## Model Selection (2025 Current Standards)

**APPROVED MODELS ONLY**: o3, o4 series, GPT-5 series
**gpt-4o series models are FORBIDDEN**

| Use‚ÄëCase                   | Primary Model                  | Alternative         | Notes                               |
| -------------------------- | ------------------------------ | ------------------- | ----------------------------------- |
| **Reasoning/Planning**     | `o4-mini`                      | `o3`                | MANDATORY for reasoning workflows   |
| **Agent Execution**        | `gpt-5-mini`                   | `gpt-5`             | MANDATORY for agent implementation  |
| **Structured Outputs**     | `gpt-5`                        | `gpt-5-mini`        | Required for JSON schema compliance |
| **Complex reasoning/code** | `gpt-5`, `o3`                  | `o4-mini`           | Context-dependent selection         |
| **Fast responses**         | `gpt-5-nano`, `gpt-5-mini`     | Efficiency priority |

**CRITICAL MODEL CONSTRAINTS**:
- **OAMAT-SD Application**: MUST use `o4-mini` (reasoning) and `gpt-5-mini` (agents)
- **Structured Outputs**: Use `gpt-5-mini` or `gpt-5` for JSON schema compliance
- **NO GPT-4O VARIANTS**: Only o3, o4, and GPT-5 series approved for use
- **Resolution**: Use appropriate model for specific requirements, document conflicts

## Latest Responses API Patterns (2025 PREFERRED)

### Standard Responses API with Structured Outputs
```python
# ‚úÖ CURRENT STANDARD - Responses API (2025)
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional

client = OpenAI()

# Structured Output with Pydantic
class ResponseSchema(BaseModel):
    analysis: str
    confidence: float
    recommendations: List[str]
    metadata: Optional[dict] = None

response = client.responses.create(
    model="gpt-5-mini",  # or "o4-mini" for reasoning
    instructions="You are a helpful assistant that provides structured analysis.",
    input="Analyze this data and provide recommendations...",
    text_format=ResponseSchema
)

# Access parsed response
result = response.output_parsed
```

### Advanced Role Hierarchy (2025 BEST PRACTICE)
```python
# ‚úÖ PRIORITY ORDER: developer > user > assistant
messages = [
    {
        "role": "developer",  # HIGHEST PRIORITY
        "content": """
        ## SYSTEM CONSTRAINTS
        - Output must be valid Python code
        - Use type hints for all functions
        - Include comprehensive error handling
        - Follow PEP 8 standards strictly

        ## FORBIDDEN ACTIONS
        - Do not use deprecated libraries
        - Do not hardcode sensitive data
        - Do not skip input validation
        """
    },
    {
        "role": "user",       # MEDIUM PRIORITY
        "content": "Create a FastAPI endpoint for user authentication with JWT tokens"
    },
    {
        "role": "assistant",  # CONTEXT ONLY
        "content": "I'll help you create a secure authentication endpoint..."
    }
]

response = client.responses.create(
    model="gpt-5-mini",
    input=messages,
    text_format=CodeResponseSchema
)
```

### Model-Specific Optimization Patterns

#### **For O3/o4-mini Models (Reasoning Focus)**
```python
# ‚úÖ O3-OPTIMIZED PATTERN - Minimal, clear prompts
response = client.responses.create(
    model="o4-mini",
    instructions="You are a system architect. Design a scalable microservices architecture.",
    input="Requirements: 100k users, real-time chat, file sharing, mobile support",
    text_format=ArchitectureSchema,
    # O3-specific parameters
    reasoning_effort="high",  # thorough analysis
    verbosity="concise"       # controlled output length
)

# ‚úÖ NO CHAIN-OF-THOUGHT for O3 - they reason internally
# ‚ùå AVOID: "Think step by step...", "Let's break this down..."
```

#### **For GPT-5 Models (Execution Focus)**
```python
# ‚úÖ GPT-5-OPTIMIZED PATTERN - Detailed, explicit guidance
response = client.responses.create(
    model="gpt-5-mini",
    instructions="""
    ## PERSISTENCE
    Continue until the user's query is completely resolved.

    ## REASONING STRATEGY
    First, analyze the requirements systematically.
    Then, consider edge cases and error conditions.
    Finally, implement with comprehensive validation.

    ## OUTPUT STANDARDS
    - Include step-by-step reasoning
    - Provide code with error handling
    - Explain design decisions
    """,
    input="Build a robust API endpoint with caching and rate limiting",
    text_format=DetailedCodeSchema
)
```

### Prompt Templates and Caching (2025 OPTIMIZATION)
```python
# ‚úÖ REUSABLE PROMPT TEMPLATES
response = client.responses.create(
    model="gpt-5-mini",
    prompt={
        "id": "pmpt_code_generator_v2",
        "version": "2.1",
        "cache": True,  # Enable prompt caching
        "variables": {
            "language": "Python",
            "framework": "FastAPI",
            "task_type": "API endpoint",
            "requirements": "authentication, validation, error handling"
        }
    },
    input="Create secure user registration endpoint",
    text_format=CodeResponseSchema
)

# ‚úÖ DYNAMIC PROMPT GENERATION
def generate_prompt_variables(task_context: dict) -> dict:
    """Generate prompt variables dynamically based on context"""
    return {
        "complexity": analyze_complexity(task_context),
        "technologies": extract_technologies(task_context),
        "constraints": identify_constraints(task_context)
    }
```

### Enhanced Structured Outputs with Validation
```python
# ‚úÖ COMPREHENSIVE SCHEMA DEFINITION
from pydantic import BaseModel, Field, validator
from enum import Enum

class CodeQuality(str, Enum):
    PRODUCTION = "production"
    DEVELOPMENT = "development"
    PROTOTYPE = "prototype"

class CodeResponse(BaseModel):
    code: str = Field(description="Complete, executable code")
    explanation: str = Field(description="Clear explanation of implementation")
    dependencies: List[str] = Field(description="Required packages and versions")
    quality_level: CodeQuality = Field(description="Code quality assessment")
    test_cases: List[str] = Field(description="Suggested test scenarios")
    security_notes: Optional[List[str]] = Field(description="Security considerations")

    @validator('code')
    def code_must_be_complete(cls, v):
        if len(v.strip()) < 10:
            raise ValueError('Code must be substantial and complete')
        return v

# ‚úÖ STRICT JSON SCHEMA (Alternative to Pydantic)
strict_schema = {
    "type": "object",
    "properties": {
        "analysis": {"type": "string", "minLength": 50},
        "confidence": {"type": "number", "minimum": 0, "maximum": 1},
        "recommendations": {
            "type": "array",
            "items": {"type": "string"},
            "minItems": 1,
            "maxItems": 10
        },
        "priority": {
            "type": "string",
            "enum": ["low", "medium", "high", "critical"]
        }
    },
    "required": ["analysis", "confidence", "recommendations", "priority"],
    "additionalProperties": False
}

response = client.responses.create(
    model="gpt-5-mini",
    input="Analyze security vulnerabilities in this code...",
    text={
        "format": {
            "type": "json_schema",
            "name": "security_analysis",
            "strict": True,
            "schema": strict_schema
        }
    }
)
```

## Advanced Tool Integration Patterns

### Built-in Tools with Custom Functions
```python
# ‚úÖ HYBRID TOOL APPROACH - Built-in + Custom
tools = [
    # Built-in capabilities
    {"type": "web_search_preview"},
    {"type": "file_search", "vector_store_ids": ["vs_123"]},

    # Custom function tools
    {
        "type": "function",
        "function": {
            "name": "validate_code_security",
            "description": "Validate code for security vulnerabilities and best practices",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Code to validate"},
                    "language": {"type": "string", "enum": ["python", "javascript", "typescript"]},
                    "security_level": {"type": "string", "enum": ["basic", "standard", "strict"]}
                },
                "required": ["code", "language"],
                "additionalProperties": False
            }
        }
    }
]

response = client.responses.create(
    model="gpt-5-mini",
    input="Review this authentication code for security issues...",
    tools=tools,
    text_format=SecurityAnalysisSchema
)
```

### Dynamic Configuration Generation (NO HARDCODED VALUES)

```python
# ‚úÖ O3 PIPELINE CONFIGURATION - Dynamic generation only
def generate_api_config(requirements: dict) -> dict:
    """Generate complete API configuration using O3 reasoning model"""

    config_response = client.responses.create(
        model="o4-mini",  # Reasoning model for configuration
        instructions="Generate optimal API configuration based on requirements",
        input=f"Requirements: {requirements}",
        text_format=APIConfigSchema
    )

    return config_response.output_parsed.model_dump()

# ‚úÖ ADAPTIVE MODEL SELECTION
def select_optimal_model(task_type: str, complexity: str) -> str:
    """Dynamically select best model for task"""

    if task_type == "reasoning" or task_type == "planning":
        return "o4-mini" if complexity == "standard" else "o3"
    elif task_type == "execution" or task_type == "coding":
        return "gpt-5-mini" if complexity == "standard" else "gpt-5"
    else:
        # Default fallback
        return "gpt-5-mini"

# ‚ùå HARDCODED CONFIG - PROHIBITED
FORBIDDEN_CONFIG = {
    "model": "gpt-5-mini",
    "temperature": 0.1,
    "max_tokens": 4000
}  # VIOLATION - No hardcoded values allowed
```

## Error Handling (NO FALLBACKS PERMITTED)

```python
import backoff
import openai
from typing import Union

@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=60)
def safe_api_call(**kwargs) -> Union[dict, None]:
    """Safe API call with proper error handling - NO FALLBACKS"""

    try:
        response = client.responses.create(**kwargs)

        # Handle specific error conditions - NO fallbacks
        if response.status == "incomplete":
            if response.incomplete_details.reason == "max_output_tokens":
                raise OpenAITokenLimitError("Token limit exceeded - increase max_tokens")
            elif response.incomplete_details.reason == "content_filter":
                raise OpenAIContentFilterError("Content filtered - revise input")
            else:
                raise OpenAIIncompleteError(f"Incomplete response: {response.incomplete_details.reason}")

        # Check for refusals
        if hasattr(response, 'output') and response.output[0].content[0].type == "refusal":
            raise OpenAIRefusalError(f"Model refused: {response.output[0].content[0].refusal}")

        return response

    except openai.AuthenticationError:
        raise ConfigurationError("Invalid API key or authentication failure")
    except openai.PermissionDeniedError:
        raise ConfigurationError("Permission denied - check model access")
    except openai.BadRequestError as e:
        raise ConfigurationError(f"Invalid request configuration: {e}")

    # NO fallback return - must succeed or fail explicitly

# ‚ùå FALLBACK PATTERNS - ABSOLUTELY PROHIBITED
def prohibited_fallback_pattern(**kwargs):
    try:
        return safe_api_call(**kwargs)
    except Exception:
        # FORBIDDEN - No fallback mechanisms allowed
        return simple_text_response()  # VIOLATION
```

## Streaming with Structured Outputs (2025 PATTERN)

```python
# ‚úÖ STRUCTURED STREAMING WITH REAL-TIME PROCESSING
def stream_structured_response(prompt: str, schema: BaseModel):
    """Stream structured response with real-time validation"""

    accumulated_data = {}

    with client.responses.stream(
        model="gpt-5-mini",
        input=prompt,
        text_format=schema,
    ) as stream:

        for event in stream:
            if event.type == "response.output_text.delta":
                # Process incremental structured data
                process_delta(event.delta, accumulated_data)

            elif event.type == "response.output_text.partial":
                # Validate partial structure
                try:
                    partial_obj = schema.model_validate_json(event.partial_json)
                    yield partial_obj
                except ValidationError:
                    # Continue streaming - validation will complete at end
                    pass

            elif event.type == "response.error":
                handle_streaming_error(event.error)

            elif event.type == "response.completed":
                final_response = stream.get_final_response()
                return final_response.output_parsed

# ‚úÖ REAL-TIME PROCESSING EXAMPLE
async def real_time_code_generation():
    async for partial_code in stream_structured_response(
        "Generate a FastAPI app with authentication",
        CodeGenerationSchema
    ):
        # Process code incrementally
        if partial_code.code:
            syntax_check(partial_code.code)
        if partial_code.dependencies:
            validate_dependencies(partial_code.dependencies)
```

## Security and Privacy (ENHANCED 2025)

```python
# ‚úÖ COMPREHENSIVE SECURITY PATTERNS
import re
from typing import Dict, Any

class DataSanitizer:
    """Enhanced data sanitization for API calls"""

    PII_PATTERNS = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
    }

    SECRET_PATTERNS = {
        'api_key': r'(?i)(api[_-]?key|apikey)[\s]*[:=][\s]*["\']?([a-zA-Z0-9_-]+)["\']?',
        'bearer_token': r'(?i)bearer[\s]+([a-zA-Z0-9_.-]+)',
        'password': r'(?i)(password|passwd|pwd)[\s]*[:=][\s]*["\']?([^\s"\']+)["\']?'
    }

    @classmethod
    def sanitize_input(cls, user_input: str) -> str:
        """Remove PII and secrets from input"""
        sanitized = user_input

        # Remove PII
        for pattern_name, pattern in cls.PII_PATTERNS.items():
            sanitized = re.sub(pattern, f'[REDACTED_{pattern_name.upper()}]', sanitized)

        # Remove secrets
        for pattern_name, pattern in cls.SECRET_PATTERNS.items():
            sanitized = re.sub(pattern, f'[REDACTED_{pattern_name.upper()}]', sanitized)

        return sanitized

def secure_api_call(user_input: str, **kwargs) -> Any:
    """Secure API call with comprehensive protection"""

    # Sanitize input
    sanitized_input = DataSanitizer.sanitize_input(user_input)

    # Log for audit (sanitized version only)
    audit_log_api_request(sanitized_input, model=kwargs.get('model'))

    # Add security headers if needed
    kwargs.setdefault('instructions', '')
    kwargs['instructions'] += "\n\nIMPORTANT: Do not generate or reference any real API keys, passwords, or sensitive data."

    try:
        response = client.responses.create(
            input=sanitized_input,
            **kwargs
        )

        # Audit successful response
        audit_log_api_response(response.id, success=True)
        return response.output_parsed

    except Exception as e:
        # Audit failed response
        audit_log_api_response(None, success=False, error=str(e))
        raise
```

## Shared Python Interfaces (MANDATORY USAGE)

**ABSOLUTE REQUIREMENT**: All agentic scripts and applications MUST use shared interfaces in `src/shared/openai_interfaces` for ALL OpenAI API access.

### Interface Hierarchy (PREFERRED ORDER):
1. **OpenAIResponsesInterface** - For new projects using Responses API
2. **OpenAIAssistantsInterface** - If Responses API not suitable
3. **Other interfaces** - As needed for specific endpoints

```python
# ‚úÖ MANDATORY PATTERN - Always use shared interfaces
from src.shared.openai_interfaces.responses_interface import OpenAIResponsesInterface
from src.shared.openai_interfaces.model_selector import OptimalModelSelector

# Initialize interface with dynamic configuration
model_selector = OptimalModelSelector()
openai_interface = OpenAIResponsesInterface(
    model_selector=model_selector,
    default_security_level="strict"
)

# Use interface for all API calls
response = openai_interface.create_structured_response(
    task_type="code_generation",
    input_data="Create a secure user authentication endpoint",
    response_schema=CodeResponseSchema,
    model_optimization="auto"  # Automatically select optimal model
)

# ‚ùå FORBIDDEN - Raw OpenAI SDK calls
import openai
client = openai.OpenAI()  # PROHIBITED in agent code
```

### Interface Implementation Requirements
```python
# ‚úÖ SHARED INTERFACE PATTERN
class OpenAIResponsesInterface:
    """Shared interface implementing all OpenAI best practices"""

    def __init__(self, security_level: str = "standard"):
        self.client = OpenAI()
        self.sanitizer = DataSanitizer()
        self.security_level = security_level

    def create_structured_response(
        self,
        task_type: str,
        input_data: str,
        response_schema: BaseModel,
        model_optimization: str = "auto"
    ):
        """Unified method implementing all best practices"""

        # 1. Model selection based on task type
        model = self._select_optimal_model(task_type)

        # 2. Security sanitization
        safe_input = self.sanitizer.sanitize_input(input_data)

        # 3. Model-specific prompt optimization
        optimized_prompt = self._optimize_prompt_for_model(model, safe_input)

        # 4. Execute with error handling
        return self._execute_with_retry(
            model=model,
            input=optimized_prompt,
            text_format=response_schema
        )
```

## MANDATORY COMPLIANCE CHECKLIST (UPDATED 2025)

### **Pre-Response Requirements**:
- [ ] Time/date checked at response start
- [ ] Current API documentation researched online
- [ ] Model constraints verified for use case
- [ ] Model-specific prompting techniques applied

### **API Implementation Requirements**:
- [ ] Responses API used (preferred over Chat Completions)
- [ ] Structured outputs implemented (no plain text responses)
- [ ] JSON schemas properly defined with strict: true
- [ ] Role hierarchy implemented (developer > user > assistant)
- [ ] Prompt templates and caching utilized when applicable

### **Model Selection and Optimization**:
- [ ] Only approved models used (O3 and GPT-5 series)
- [ ] Model selected dynamically based on task type
- [ ] O3 models: Clear, minimal prompts with no chain-of-thought
- [ ] GPT-5 models: Detailed, explicit instructions with reasoning guidance
- [ ] Reasoning effort parameter set for o4-mini when applicable

### **Security and Quality Requirements**:
- [ ] Shared interfaces used exclusively (no raw SDK calls)
- [ ] Input sanitization implemented (PII and secrets removal)
- [ ] No fallback mechanisms implemented
- [ ] Error handling without fallbacks (fail-fast approach)
- [ ] Security patterns implemented (audit logging, redaction)

### **Dynamic Configuration Requirements**:
- [ ] Dynamic configuration generation (O3 based)
- [ ] No hardcoded values (models, parameters, schemas)
- [ ] Evidence-based model selection documented
- [ ] Configuration adapts to task requirements

### **Advanced Features**:
- [ ] Streaming implemented for long-running tasks
- [ ] Built-in tools utilized where appropriate
- [ ] Real-time validation for structured outputs
- [ ] Performance optimization applied

## üîê MODEL CONSTRAINT ENFORCEMENT (IRONCLAD)

**ABSOLUTE PROHIBITIONS - ZERO TOLERANCE**:

### **‚ùå UNAUTHORIZED MODELS (IMMEDIATE REJECTION)**
```python
# üö® FORBIDDEN MODELS - INSTANT VIOLATION
"gpt-4o"              # PROHIBITED
"gpt-4o-mini"         # PROHIBITED
"gpt-4o-2024-*"       # PROHIBITED
"claude-3"            # PROHIBITED
"claude-*"            # PROHIBITED
"anthropic/*"         # PROHIBITED
"gemini-pro"          # PROHIBITED
"gemini-*"            # PROHIBITED
"google/*"            # PROHIBITED
"llama-*"             # PROHIBITED
"mistral-*"           # PROHIBITED

# üö® FORBIDDEN PATTERNS
MODEL_OPTIONS = ["gpt-4o", "o4-mini"]  # VIOLATION - unauthorized model in list
if gpt_4o_available: use_gpt_4o()      # VIOLATION - conditional unauthorized usage
backup_model = "gpt-4o-mini"           # VIOLATION - fallback to unauthorized model
```

### **‚úÖ APPROVED MODELS ONLY**
```python
# ‚úÖ MANDATORY APPROVED MODELS
"o4-mini"             # ‚úÖ APPROVED - Reasoning
"o3"                  # ‚úÖ APPROVED - Advanced reasoning
"gpt-5-mini"          # ‚úÖ APPROVED - Agent execution
"gpt-5"               # ‚úÖ APPROVED - Advanced execution
"gpt-5-nano"          # ‚úÖ APPROVED - Fast responses

# ‚úÖ APPROVED CONFIGURATION PATTERNS
REASONING_MODEL = "o4-mini"         # ‚úÖ CORRECT
EXECUTION_MODEL = "gpt-5-mini"      # ‚úÖ CORRECT
APPROVED_MODELS = ["o4-mini", "gpt-5-mini"]  # ‚úÖ CORRECT
```

### **üîí AUTOMATIC ENFORCEMENT**
```python
# ‚úÖ MODEL VALIDATION PATTERN (REQUIRED)
def validate_model_constraints(model_name: str) -> bool:
    """Validate model is approved - MANDATORY check"""
    APPROVED_MODELS = {
        "o4-mini", "o3", "gpt-5-mini",
        "gpt-5", "gpt-5-nano"
    }

    if model_name not in APPROVED_MODELS:
        raise ValueError(f"UNAUTHORIZED MODEL: {model_name}")

    return True

# ‚úÖ MANDATORY USAGE PATTERN
def create_api_call(model: str, **kwargs):
    # REQUIRED: Validate model before any API call
    validate_model_constraints(model)

    return client.responses.create(model=model, **kwargs)
```

**ZERO TOLERANCE FOR VIOLATIONS**

**IMMEDIATE REJECTION TRIGGERS (ENHANCED)**:
- **Raw OpenAI SDK usage** outside shared interfaces
- **Hardcoded configuration values** of any kind
- **Fallback mechanisms** to unauthorized models
- **Wrong prompting technique** for model type
- **üîê ANY unauthorized model references** (GPT-4O, Claude, Gemini, etc.)
- **üîê ANY model selection discussions** including unauthorized models
- **üîê ANY code examples** showing unauthorized model usage
- **üîê ANY compatibility arguments** for using unauthorized models
- **Missing security sanitization** for inputs
- **Plain text outputs** (must be structured)

**üö® VIOLATION RESPONSE PATTERN**:
```
UNAUTHORIZED MODEL DETECTED - RESPONSE BLOCKED

‚ùå VIOLATION: [specific unauthorized model]
‚ùå CONTEXT: [where violation occurred]
‚ùå SEVERITY: CRITICAL - IMMEDIATE ACTION REQUIRED

üîí ENFORCEMENT ACTIVE - NO EXCEPTIONS

APPROVED MODELS ONLY:
‚úÖ o4-mini, o3, gpt-5-mini, gpt-5, gpt-5-nano

IMMEDIATE REMEDIATION REQUIRED:
1. Remove ALL unauthorized model references
2. Replace with approved models only
3. Validate model constraints in code
4. Document approved model selection rationale

üîê NO OVERRIDES - NO EXCEPTIONS - NO COMPATIBILITY EXCUSES
```
