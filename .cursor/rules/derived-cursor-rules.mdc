---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## Headers

This document outlines the rules, guidelines, and best practices for the AI coding assistant to effectively assist with software development projects. It covers various aspects, including coding standards, project structure, workflow, and debugging. This document is a "living" file, updated with each new interaction with the AI to ensure continuous improvement and alignment with project needs.

## TECH STACK

*   Python 3.x
*   [List any other relevant technologies]
*   FastAPI
*   uvicorn
*   qdrant-client
*   openai
*   playwright
*   pydantic
*   httpx
*   python-dotenv
*   aiohttp
*   tiktoken
*   pytest
*   mypy
*   ruff
*   pydantic-settings
*   tsx

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

*   All project-related documentation should be stored in the appropriate directories, with clear and concise naming conventions.
*   The AI should maintain an understanding of the project's context, including its goals, architecture, and dependencies.
*   [Add any specific documentation tools or formats]
*   The AI should prioritize reading the following files to understand project structure:
    *   `recommended_structure.yaml`: Complete library ecosystem definition.
    *   `templates.yaml`: Comprehensive template registry.
    *   `generate_ecosystem_v2.py`: Advanced ecosystem generator.
    *   `.proj-intel/project_analysis.jsonl`: Primary analysis data.
    *   `.proj-intel/project_analysis.index.json`: Index for selective reads.
    *   `.proj-intel/reverse_index.json`: O(1) path lookup.
    *   `agent_architecture.jsonl`: Agent class definitions.
    *   `INTEGRATIONS_DIRECTORY_TREE.md`: Detailed integration mapping.
*   The `.mcp.json` file in the project root of this project defines the tools the claude code app has available and is located at: `/home/opsvi/master_root/.mcp.json`.

## CODING STANDARDS

*   Adhere to PEP 8 guidelines for Python code.
*   Write clear, concise, and well-commented code.
*   Use meaningful variable and function names.
*   Follow established design patterns and architectural principles.
*   Ensure code is modular and reusable.
*   Handle errors gracefully using try-except blocks.
*   Always include comprehensive docstrings for all functions and classes.
*   Follow SOLID principles and DRY approach.
*   Use Async/await throughout for scalability.
*   Use comprehensive type hints with MyPy.
*   Use Pydantic for configuration validation.
*   Use structured logging with correlation IDs.
*   Design components with BaseComponent inheritance.
*   Use dependency injection for loose coupling.
*   Implement event-driven communication between components.
*   Design for configuration-driven behavior.
*   Implement input validation and sanitization.
*   Use authentication and authorization mechanisms.
*   Implement audit logging and security event tracking.
*   Integrate vulnerability scanning.

## WORKFLOW & RELEASE RULES

*   Follow a Git-based workflow for version control.
*   Use feature branches for new development.
*   Create pull requests for code review.
*   All code must be reviewed and approved before merging.
*   Implement automated testing and CI/CD pipelines.
*   Always use `shell_exec` instead of `run_terminal_cmd` for executing commands.
*   When committing changes to git, include a special note in the description if the claude code MCP server is working properly.
*   When I ask you to generate a diagram, I expect it to be rendered inline in the chat session.

## DEBUGGING

*   Use logging for debugging purposes.
*   Include detailed error messages.
*   Utilize debugging tools and techniques to identify and resolve issues.

## STYLE & FORMATTING

*   Use Markdown for documentation.
*   Format code blocks with proper syntax highlighting.
*   Keep lines concise for readability.
*   When presenting the chat name in the matrix, strip off the date and file extension for cleaner presentation.
*   **CRITICAL CONTRAST RULE**: Dark text must be overlaid on a light background or vice versa. NEVER light on light or dark on dark.
*   **MANDATORY CONTRAST CHECK**: Always verify all text-background combinations for sufficient contrast.

## CHAT LOG PROCESSING SCRIPTS

*   Two Python scripts are available for processing chat logs:
    *   `collapse_chat_log.py`: Collapses tool calls into summary lines with truncation.
    *   `log_stripper.py`: Completely removes tool calls into summary lines with truncation.
*   The `log_stripper.py` script removes thoughts and tool calls completely from a chat log to create a clean version with only user and assistant messages for better readability.
*   When using the log processing scripts:
    *   The `input` argument specifies the input chat log file.
    *   The `output` argument specifies the output file.

## CHAT LOG ANALYSIS

*   A Python script `chat_log_analyzer.py` is available for analyzing chat logs. It reports:
    *   Number of lines
    *   Number of user prompts (detects formats like `_**User**_`, `User:`, `User input:`, `Human:`, `<user>`, `User says:`, etc.)
    *   Number of assistant responses (detects formats like `_**Assistant**_`, `Assistant:`, `AI:`, `<assistant>`, `Response:`, `Bot:`, etc.)
    *   Number of tool calls without a preceding `<think>` tag
    *   Number of thoughts `<think> </think> </think>` pairs
    *   Tool Calls per Line (Percentage)
    *   Thoughts per Line (Percentage)
    *   Assistant/User Ratio

*   A Python script `batch_analyze.py` is available for batch processing of chat logs. It runs the `chat_log_analyzer.py` script on all `.md` files in a specified directory and generates a summary report.

*   A Python script `metrics_matrix.py` is available for displaying chat log metrics in a matrix format for easy comparison.
    *   The chat name is stripped of the date and file extension for cleaner presentation.
    *   The matrix is sorted by the number of lines in descending order.
    *   The Tool % and A/U columns are dropped from the matrix for a more focused view.

*   A Python script `comprehensive_analyzer.py` is available for comprehensive chat log analysis and formatting. It reports:
    *   Chat: The chat log name.
    *   Lines (Before): Total lines before formatting.
    *   Lines (After): Total lines after formatting.
    *   Chars: Total characters.
    *   Words: Total words.
    *   Tokens: Total tokens.
    *   Tool: Number of tool calls.
    *   Prompt: Number of user/assistant prompt/response cycles. 1 response to 1 prompt = 1 cycle
    *   Think: Number of `<think> </think> </think>` pairs
    *   Blank: Number of blank lines
    *   Tools Used: Unique list of tools used in the chat file, comma-separated.
    *   It also creates a formatted copy of each chat log in a sub-folder named `_formatted` with the following rules:
        *   No more than one consecutive blank line.
        *   Same for lines that only have `---`.
        *   Tool calls `<details> </details>` are collapsed into a single line and carriage returns are replaced with `||`.
        *   Think blocks `<think> </think> </think>` are collapsed into a single line and carriage returns are replaced with `||`.
    *   Matrix must be presented as a table in the chat response
    *   The `.csv` output needs to be written to `/tmp`.

*   A Python script `enhanced_analyzer.py` is available for further analysis and formatted reporting of chat logs. It generates multiple views of the data, including:
    *   Executive Summary
    *   File Categories Report
    *   Tool Usage Analysis
    *   Content Quality Report
    *   Detailed Data (existing CSV format)

## LIBRARY MIGRATION STRATEGY

### General Strategy
Leverage existing, proven interfaces and implementations from the `agent_world/src/shared` directory as a foundation for the `libs/` directory. Adapt these implementations to the OPSVI template structure while maintaining compatibility during the transition.

### Priority Implementation Order
```yaml
Phase 1 (Immediate):
  - opsvi-llm: Migrate OpenAI interfaces
  - opsvi-http: Migrate HTTP client interface
  - opsvi-data: Migrate database interfaces

Phase 2 (Short-term):
  - opsvi-rag: Implement vector store adapters
  - opsvi-orchestration: Migrate CrewAI/LangGraph interfaces
  - opsvi-memory: Implement caching systems

Phase 3 (Medium-term):
  - opsvi-agents: Implement multi-agent framework
  - opsvi-monitoring: Add observability
  - opsvi-security: Add security layers
```

### Specific Migration Tasks
```python
# OpenAI Interfaces → opsvi-llm
intake/custom/agent_world/src/shared/openai_interfaces/
  → libs/opsvi-llm/opsvi_llm/providers/openai_provider.py

# HTTP Client → opsvi-http  
intake/custom/agent_world/src/shared/interfaces/http/
  → libs/opsvi-http/opsvi_http/client/base.py

# Database Interfaces → opsvi-data
intake/custom/agent_world/src/shared/interfaces/database/
  → libs/opsvi-data/opsvi_data/providers/graph/arangodb_adapter.py

# MCP Integration → opsvi-orchestration
intake/custom/agent_world/src/shared/mcp/
  → libs/opsvi-orchestration/opsvi_orchestration/integrations/research_tools/
```

### Architecture Improvements

Enhance OPSVI with Agent World Patterns:
*   Collaboration Framework: Integrate into opsvi-agents
*   Intelligent Relevance Scoring: Add to opsvi-rag
*   Pattern-based Agent Selection: Implement in opsvi-orchestration
*   Performance Metrics: Add to opsvi-monitoring

### Agent World Architecture Assessment

The `agent_world` implementation represents a sophisticated, production-ready multi-agent orchestration platform with distinct layers.

#### Core Infrastructure (Fully Implemented)

1.  **API Layer** (`src/api/`):
    *   FastAPI application with comprehensive REST endpoints.
    *   WebSocket support for real-time updates.
    *   Pipeline execution, status tracking, artifact management.
    *   Health checks and queue monitoring.

2.  **Orchestration Engine** (`src/orchestrator/`):
    *   Meta-orchestrator for pipeline coordination.
    *   DAG-based workflow execution.
    *   Task registry and dependency management.
    *   Celery integration for distributed task processing.

3.  **Memory System** (`src/memory/`):
    *   Neo4j graph database for lineage tracking.
    *   Comprehensive schema for projects, runs, tasks, artifacts.
    *   Query system for complex graph operations.

4.  **Worker System** (`src/workers/`):
    *   Celery-based task execution.
    *   8 specialized task types (plan, spec, research, code, test, validate, document, critic).
    *   Metrics tracking and performance monitoring.

#### Application Layer (Extensive Implementation)

1.  **Agent Hub** (`src/applications/agent_hub/`):
    *   Multi-agent system with 10+ specialized agents.
    *   Agent discovery and service registration.
    *   MCP (Model Context Protocol) compliance.
    *   Dashboard and visualization tools.

2.  **SpecStory Intelligence** (`src/applications/specstory_intelligence/`):
    *   Advanced conversation intelligence.
    *   Atomic parsing and conceptual analysis.
    *   Database storage and pipeline integration.
    *   1,500+ lines of sophisticated analysis code.

3.  **Code Generation** (`src/applications/code_gen/`):
    *   AI-powered code generation pipeline.
    *   Test generation and validation.
    *   Documentation generation.
    *   Rate limiting and audit logging.

4.  **Multi-Agent Orchestration** (`src/applications/multi_agent_orchestration/`):
    *   Workflow orchestration system.
    *   Research and task agents.
    *   Communication broker.
    *   Demo and production modes.

#### Shared Infrastructure (Production-Ready)

1.  **OpenAI Interfaces** (20+ files, 1,000+ lines)
    *   Complete OpenAI API coverage.
    *   Authentication and error handling.
    *   Specialized interfaces for each API endpoint.

2.  **MCP Integration** (15+ files, 2,000+ lines)
    *   Brave Search, Firecrawl, ArXiv, Context7.
    *   Research workflow tools.
    *   Neo4j MCP client.
    *   Intelligent relevance scoring.

3.  **Database Interfaces** (10+ files, 3,000+ lines)
    *   ArangoDB (1,112 lines - comprehensive).
    *   PostgreSQL, MySQL, Redis, Elasticsearch.
    *   S3 and consolidated operations.

4.  **Collaboration Framework** (849 lines)
    *   SpecStory Intelligence Pipeline integration.
    *   Agent Hub communication.
    *   Pattern-based agent selection.
    *   Performance metrics and learning.

### Key Strengths & Innovations

#### Production-Ready Architecture

*   **Scalable**: Celery-based distributed processing.
*   **Observable**: Comprehensive logging and metrics.
*   **Reliable**: Error handling and retry mechanisms.
*   **Maintainable**: Well-structured codebase with clear separation.

#### Advanced Multi-Agent System

*   **10+ Specialized Agents**: dev_agent, sentinel, kb_updater, graph_analyst, etc.
*   **Intelligent Routing**: Pattern-based agent selection.
*   **Collaborative Problem Solving**: Multi-agent orchestration.
*   **Learning & Adaptation**: Performance tracking and improvement.

#### Comprehensive Integration

*   **MCP Ecosystem**: Full Model Context Protocol implementation.
*   **Database Support**: Multiple database types with unified interfaces.
*   **API Coverage**: Complete OpenAI API integration.
*   **Research Tools**: Advanced web scraping and analysis.

#### Sophisticated Intelligence Pipeline

*   **Conversation Intelligence**: Deep analysis of chat patterns.
*   **Atomic Parsing**: Granular content analysis.
*   **Conceptual Synthesis**: High-level pattern recognition.
*   **Lineage Tracking**: Complete audit trail

### Updated Migration Strategy

The peer codebase analysis reveals **exceptional coverage** for the OPSVI libs/ architecture. With proper migration from the peer codebases, we can achieve **88% coverage** of the required OPSVI libraries. The new peer codebases to investigate are:

*   `/home/opsvi/master_root/intake/custom/asea`
*   `/home/opsvi/master_root/intake/custom/auto_forge`
*   `/home/opsvi/master_root/intake/custom/docRuleGen`
*   `/home/opsvi/master_root/intake/custom/master`

#### Updated Priorities

1.  Focus on building the **Foundation Layer** first, then systematically address the **Service Layer** gaps, using industry best practices and OPSVI architecture patterns.

2.  Start with **Phase 1 (Core Infrastructure)** to establish the foundation, then proceed systematically through the phases. This approach maximizes value while minimizing risk.

### Updated Agent World Architecture Assessment

The peer codebase analysis reveals **exceptional coverage** for the OPSVI libs/ architecture. With proper migration from the peer codebases, we can achieve **88% coverage** of the required OPSVI libraries.

### New Peer Codebases

The new peer codebases to investigate are:

*   `/home/opsvi/master_root/intake/custom/asea`
*   `/home/opsvi/master_root/intake/custom/auto_forge`
*   `/home/opsvi/master_root/intake/custom/docRuleGen`
*   `/home/opsvi/master_root/intake/custom/master`
*   `/home/opsvi/master_root/intake/custom/graphRAG`
*   `/home/opsvi/master_root/intake/custom/graphiti`
*   `/home/opsvi/master_root/intake/custom/graph_rag`
*   `/home/opsvi/master_root/intake/custom/graph_rag2`
*   `/home/opsvi/master_root/intake/custom/SKG_Cursor`
*   `/home/opsvi/master_root/intake/custom/ide_contex_visualization`

## Actionable Migration Guidance

The agent_world is eventually going away, so the following steps need to be taken:

1.  Create a map of agent\_world -&gt; master\_root/libs
2.  Port into this environment to get things online and ready faster
3.  Use proven code

## Using Shell Exec

Always use `shell_exec`, not `run_terminal_cmd`

When extracting or manipulating files, the AI should:
*   Log the current directory.
*   List all files in the target directory.
*   Check the git status if the target directory is a git repository.
*   Request human confirmation before performing actions, especially if multiple files match the criteria.
*   If multiple files match the criteria, use a loop or wildcard to process all matching files.
*   Ensure the target directory exists before attempting to change into it.
*   After extracting files, the AI should ask if the user wants to remove the archive or list the extracted contents.
*   If a git lock file is encountered, attempt to remove it using `rm -f .git/index.lock`.

## Consult Agent Usage Rules

### Core Purpose

This rule governs how to properly integrate and use consult agents for development tasks requiring expert guidance, complex planning, production-ready code generation, or architectural decision-making. **Use when INTEGRATING CONSULT AGENTS to ENFORCE PATTERNS for QUALITY DEVELOPMENT**

### Key Requirements

#### 1. Pattern Recognition Triggers

The rule specifies WHEN to use consult agents:
- Development tasks requiring expert guidance
- Complex implementation planning
- Production-ready code generation
- Architectural decision-making

#### 2. Session Management Strategy

- **Context Preservation**: Use specific, descriptive session IDs for related tasks
- **Session Tracking**: Maintain session state on requesting agent side
- **Context Quality**: Same session for iterative development, new session for unrelated tasks
- **Memory Architecture**: 1-hour timeout, last 3 exchanges preserved, automatic cleanup

#### 3. Parameter Optimization

- **Auto-Selection Trust**: Let intelligent gatekeeper determine optimal parameters
- **Model Selection**: gpt-5-nano for code, o3 for architecture, gpt-5-mini for documentation
- **Quality Control**: Enable critic for production code, disable for simple queries
- **Iteration Strategy**: 1 for simple tasks, 2 for complex, 3 for critical systems

#### 4. Prompt Engineering Patterns

- **Code Generation**: TASK + LANGUAGE/RUNTIME + INPUTS + CONSTRAINTS + OUTPUT_FORMAT
- **Architecture Planning**: ROLE + OBJECTIVE + CONTEXT + DELIVERABLES + FORMAT + RULES
- **Documentation**: TARGET_AUDIENCE + DOC_TYPE + PRODUCT_CONTEXT + MUST_INCLUDE + TONE

Always use `shell_exec`, not `run_terminal_cmd`

#### New Rule: Parameter Optimization
- **Auto-Selection Trust**: Let intelligent gatekeeper determine optimal parameters
- **Model Selection**: gpt-5-nano for code, o3 for architecture, gpt-5-mini for documentation
- **Quality Control**: Enable critic for production code, disable for simple queries
- **Iteration Strategy**: 1 for simple tasks, 2 for complex, 3 for critical systems

#### New Rule: Pattern Recognition Triggers

The rule specifies WHEN to use consult agents:
- Development tasks requiring expert guidance
- Complex implementation planning
- Production-ready code generation
- Architectural decision-making

#### New Rule: Prompt Engineering Patterns

- **Code Generation**: TASK + LANGUAGE/RUNTIME + INPUTS + CONSTRAINTS + OUTPUT_FORMAT
- **Architecture Planning**: ROLE + OBJECTIVE + CONTEXT + DELIVERABLES + FORMAT + RULES
- **Documentation**: TARGET_AUDIENCE + DOC_TYPE + PRODUCT_CONTEXT + MUST_INCLUDE + TONE

Always use `shell_exec`, not `run_terminal_cmd`

### Example Consult Agent Call
When using a consult agent, ensure the following parameters are included in the request:
*   Analyze the rule in detail
*   Follow the instructions
*   Generate a prompt
*   Include all relevant files
*   Response format: dynamic
*   Iteration: 2
*   Critic: yes
*   Set a unique session ID
*   Project Intel: true
*   No 3.5 OpenAI models
*   Use the large embedding model from OpenAI
*   Ensure a satisfactorily defined DB schema for the vector DB
*   Ensure an output schema defined for the research packages generated for storage

## OpenAI API Integration Rules (2025)

### MANDATORY PRE-RESPONSE REQUIREMENTS

**BEFORE ANY OPENAI API INTEGRATION**:

#### 1. TIME AND DATE CHECK
```python
# MANDATORY: Check current time/date at response start
from mcp_time_current_time import current_time
current_time = current_time(format="YYYY-MM-DD HH:mm:ss")
```

#### 2. ONLINE RESEARCH REQUIREMENT
- **RESEARCH CURRENT API VERSIONS**: OpenAI API changes frequently (2025 updates)
- **VERIFY MODEL AVAILABILITY**: Confirm model names and capabilities
- **CHECK SYNTAX UPDATES**: API syntax evolves rapidly (Responses API vs Chat Completions)
- **VALIDATE PRICING**: Confirm current token costs and limits
- **PROMPTING BEST PRACTICES**: Research latest model-specific optimization techniques

#### 3. EVIDENCE-BASED API SELECTION
- Base model choices on CURRENT documentation
- Verify feature availability through research
- No assumptions about API capabilities
- Test model-specific prompting strategies

### Model Selection (2025 Current Standards)

**APPROVED MODELS ONLY**: o3, o4 series, GPT-5 series
**gpt-4o series models are FORBIDDEN**
**NO GPT-3.5 MODELS**

| Use‑Case                   | Primary Model                  | Alternative         | Notes                               |
| ----- | --- | ---- | ----- |
| **Reasoning/Planning**     | `o4-mini`                      | `o3`                | MANDATORY for reasoning workflows   |
| **Agent Execution**        | `gpt-5-mini`                   | `gpt-5`             | MANDATORY for agent implementation  |
| **Structured Outputs**     | `gpt-5`                        | `gpt-5-mini`        | Required for JSON schema compliance |
| **Complex reasoning/code** | `gpt-5`, `o3`                  | `o4-mini`           | Context-dependent selection         |
| **Fast responses**         | `gpt-5-nano`, `gpt-5-mini`     | Efficiency priority |

**CRITICAL MODEL CONSTRAINTS**:
- **OAMAT-SD Application**: MUST use `o4-mini` (reasoning) and `gpt-5-mini` (agents)
- **Structured Outputs**: Use `gpt-5-mini` or `gpt-5` for JSON schema compliance
- **NO GPT-4O VARIANTS**: Only o3, o4, and GPT-5 series approved for use
- **NO GPT-3.5 MODELS**
- **Resolution**: Use appropriate model for specific requirements, document conflicts

### Latest Responses API Patterns (2025 PREFERRED)

#### Standard Responses API with Structured Outputs
```python
# ✅ CURRENT STANDARD - Responses API (2025)
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional

client = OpenAI()

# Structured Output with Pydantic
class ResponseSchema(BaseModel):
    analysis: str
    confidence: float
    recommendations: List[str]
    metadata: Optional[dict] = None

response = client.responses.create(
    model="gpt-5-mini",  # or "o4-mini" for reasoning
    instructions="You are a helpful assistant that provides structured analysis.",
    input="Analyze this data and provide recommendations...",
    text_format=ResponseSchema
)

# Access parsed response
result = response.output_parsed
```

#### Advanced Role Hierarchy (2025 BEST PRACTICE)
```python
# ✅ PRIORITY ORDER: developer > user > assistant
messages = [
    {
        "role": "developer",  # HIGHEST PRIORITY
        "content": """
        ## SYSTEM CONSTRAINTS
        - Output must be valid Python code
        - Use type hints for all functions
        - Include comprehensive error handling
        - Follow PEP 8 standards strictly

        ## FORBIDDEN ACTIONS
        - Do not use deprecated libraries
        - Do not hardcode sensitive data
        - Do not skip input validation
        """
    },
    {
        "role": "user",       # MEDIUM PRIORITY
        "content": "Create a FastAPI endpoint for user authentication with JWT tokens"
    },
    {
        "role": "assistant",  # CONTEXT ONLY
        "content": "I'll help you create a secure authentication endpoint..."
    }
]

response = client.responses.create(
    model="gpt-5-mini",
    input=messages,
    text_format=CodeResponseSchema
)
```

### Model-Specific Optimization Patterns

#### **For O3/o4-mini Models (Reasoning Focus)**
```python
# ✅ O3-OPTIMIZED PATTERN - Minimal, clear prompts
response = client.responses.create(
    model="o4-mini",
    instructions="You are a system architect. Design a scalable microservices architecture.",
    input="Requirements: 100k users, real-time chat, file sharing, mobile support",
    text_format=ArchitectureSchema,
    # O3-specific parameters
    reasoning_effort="high",  # thorough analysis
    verbosity="concise"       # controlled output length
)

# ✅ NO CHAIN-OF-THOUGHT for O3 - they reason internally
# ❌ AVOID: "Think step by step...", "Let's break this down..."
```

#### **For GPT-5 Models (Execution Focus)**
```python
# ✅ GPT-5-OPTIMIZED PATTERN - Detailed, explicit guidance
response = client.responses.create(
    model="gpt-5-mini",
    instructions="""
    ## PERSISTENCE
    Continue until the user's query is completely resolved.

    ## REASONING STRATEGY
    First, analyze the requirements systematically.
    Then, consider edge cases and error conditions.
    Finally, implement with comprehensive validation.

    ## OUTPUT STANDARDS
    - Include step-by-step reasoning
    - Provide code with error handling
    - Explain design decisions
    """,
    input="Build a robust API endpoint with caching and rate limiting",
    text_format=DetailedCodeSchema
)
```

### Prompt Templates and Caching (2025 OPTIMIZATION)
```python
# ✅ REUSABLE PROMPT TEMPLATES
response = client.responses.create(
    model="gpt-5-mini",
    prompt={
        "id": "pmpt_code_generator_v2",
        "version": "2.1",
        "cache": True,  # Enable prompt caching
        "variables": {
            "language": "Python",
            "framework": "FastAPI",
            "task_type": "API endpoint",
            "requirements": "authentication, validation, error handling"
        }
    },
    input="Create secure user registration endpoint",
    text_format=CodeResponseSchema
)

# ✅ DYNAMIC PROMPT GENERATION
def generate_prompt_variables(task_context: dict) -> dict:
    """Generate prompt variables dynamically based on context"""
    return {
        "complexity": analyze_complexity(task_context),
        "technologies": extract_technologies(task_context),
        "constraints": identify_constraints(task_context)
    }
```

### Enhanced Structured Outputs with Validation
```python
# ✅ COMPREHENSIVE SCHEMA DEFINITION
from pydantic import BaseModel, Field, validator
from enum import Enum

class CodeQuality(str, Enum):
    PRODUCTION = "production"
    DEVELOPMENT = "development"
    PROTOTYPE = "prototype"

class CodeResponse(BaseModel):
    code: str = Field(description="Complete, executable code")
    explanation: str = Field(description="Clear explanation of implementation")
    dependencies: List[str] = Field(description="Required packages and versions")
    quality_level: CodeQuality = Field(description="Code quality assessment")
    test_cases: List[str] = Field(description="Suggested test scenarios")
    security_notes: Optional[List[str]] = Field(description="Security considerations")

    @validator('code')
    def code_must_be_complete(cls, v):
        if len(v.strip()) < 10:
            raise ValueError('Code must be substantial and complete')
        return v

# ✅ STRICT JSON SCHEMA (Alternative to Pydantic)
strict_schema = {
    "type": "object",
    "properties": {
        "analysis": {"type": "string", "minLength": 50},
        "confidence": {"type": "number", "minimum": 0, "maximum": 1},
        "recommendations": {
            "type": "array",
            "items": {"type": "string"},
            "minItems": 1,
            "maxItems": 10
        },
        "priority": {
            "type": "string",
            "enum": ["low", "medium", "high", "critical"]
        }
    },
    "required": ["analysis", "confidence", "recommendations", "priority"],
    "additionalProperties": False
}

response = client.responses.create(
    model="gpt-5-mini",
    input="Analyze security vulnerabilities in this code...",
    text={
        "format": {
            "type": "json_schema",
            "name": "security_analysis",
            "strict": True,
            "schema": strict_schema
        }
    }
)
```

## Advanced Tool Integration Patterns

### Built-in Tools with Custom Functions
```python
# ✅ HYBRID TOOL APPROACH - Built-in + Custom
tools = [
    # Built-in capabilities
    {"type": "web_search_preview"},
    {"type": "file_search", "vector_store_ids": ["vs_123"]},

    # Custom function tools
    {
        "type": "function",
        "function": {
            "name": "validate_code_security",
            "description": "Validate code for security vulnerabilities and best practices",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Code to validate"},
                    "language": {"type": "string", "enum": ["python", "javascript", "typescript"]},
                    "security_level": {"type": "string", "enum": ["basic", "standard", "strict"]}
                },
                "required": ["code", "language"],
                "additionalProperties": False
            }
        }
    }
]

response = client.responses.create(
    model="gpt-5-mini",
    input="Review this authentication code for security issues...",
    tools=tools,
    text_format=SecurityAnalysisSchema
)
```

### Dynamic Configuration Generation (NO HARDCODED VALUES)

```python
# ✅ O3 PIPELINE CONFIGURATION - Dynamic generation only
def generate_api_config(requirements: dict) -> dict:
    """Generate complete API configuration using O3 reasoning model"""

    config_response = client.responses.create(
        model="o4-mini",  # Reasoning model for configuration
        instructions="Generate optimal API configuration based on requirements",
        input=f"Requirements: {requirements}",
        text_format=APIConfigSchema
    )

    return config_response.output_parsed.model_dump()

# ✅ ADAPTIVE MODEL SELECTION
def select_optimal_model(task_type: str, complexity: str) -> str:
    """Dynamically select best model for task"""

    if task_type == "reasoning" or task_type == "planning":
        return "o4-mini" if complexity == "standard" else "o3"
    elif task_type == "execution" or task_type == "coding":
        return "gpt-5-mini" if complexity == "standard" else "gpt-5"
    else:
        # Default fallback
        return "gpt-5-mini"

# ❌ HARDCODED CONFIG - PROHIBITED
FORBIDDEN_CONFIG = {
    "model": "gpt-5-mini",
    "temperature": 0.1,
    "max_tokens": 4000
}  # VIOLATION - No hardcoded values allowed
```

## Error Handling (NO FALLBACKS PERMITTED)

```python
import backoff
import openai
from typing import Union

@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=60)
def safe_api_call(**kwargs) -> Union[dict, None]:
    """Safe API call with proper error handling - NO FALLBACKS"""

    try:
        response = client.responses.create(**kwargs)

        # Handle specific error conditions - NO fallbacks
        if response.status == "incomplete":
            if response.incomplete_details.reason == "max_output_tokens":
                raise OpenAITokenLimitError("Token limit exceeded - increase max_tokens")
            elif response.incomplete_details.reason == "content_filter":
                raise OpenAIContentFilterError("Content filtered - revise input")
            elif response.incomplete_details.reason == "max_model_tokens":
                raise OpenAITokenLimitError("Model token limit exceeded - use a model with higher context")
            else:
                raise OpenAIIncompleteError(f"Incomplete response: {response.incomplete_details.reason}")

        # Check for refusals
        if hasattr(response, 'output') and response.output[0].content[0].type == "refusal":
            raise OpenAIRefusalError(f"Model refused: {response.output[0].content[0].refusal}")

        return response

    except openai.AuthenticationError:
        raise ConfigurationError("Invalid API key or authentication failure")
    except openai.PermissionDeniedError:
        raise ConfigurationError("Permission denied - check model access")
    except openai.BadRequestError as e:
        raise ConfigurationError(f"Invalid request configuration: {e}")

    # NO fallback return - must succeed or fail explicitly

# ❌ FALLBACK PATTERNS - ABSOLUTELY PROHIBITED
def prohibited_fallback_pattern(**kwargs):
    try:
        return safe_api_call(**kwargs)
    except Exception:
        # FORBIDDEN - No fallback mechanisms allowed
        return simple_text_response()  # VIOLATION
```

## Streaming with Structured Outputs (2025 PATTERN)

```python
# ✅ STRUCTURED STREAMING WITH REAL-TIME PROCESSING
def stream_structured_response(prompt: str, schema: BaseModel):
    """Stream structured response with real-time validation"""

    accumulated_data = {}

    with client.responses.stream(
        model="gpt-5-mini",
        input=prompt,
        text_format=schema,
    ) as stream:

        for event in stream:
            if event.type == "response.output_text.delta":