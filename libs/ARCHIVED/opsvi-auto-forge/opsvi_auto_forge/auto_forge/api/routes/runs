"""Run management routes for the autonomous software factory."""

import logging
from typing import Dict, Any, List, Optional
from uuid import UUID

from fastapi import APIRouter, HTTPException, Depends, Query, BackgroundTasks
from prometheus_client import Counter, Histogram

from auto_forge.config.models import Run, RunResponse, TaskRecord
from memory.graph.client import Neo4jClient
from .websockets import broadcast_run_update

logger = logging.getLogger(__name__)


async def test_background_task(run_id: str) -> None:
    """Simple test background task to verify FastAPI background tasks work."""
    logger.info(f"🧪 TEST BACKGROUND TASK EXECUTED for run: {run_id}")
    import time
    time.sleep(1)  # Small delay to ensure it's actually executed
    logger.info(f"🧪 TEST BACKGROUND TASK COMPLETED for run: {run_id}")


async def start_pipeline_execution(run_id: str, neo4j_client: Neo4jClient) -> None:
    """Background task to start pipeline execution."""
    logger.info(f"🔵 BACKGROUND TASK STARTED for run: {run_id}")

    try:
        logger.info(f"🔵 Step 1: Updating run status to running for {run_id}")
        # Update run status to running
        await neo4j_client.update_run_status(run_id, "running")

        # Broadcast run started event
        await broadcast_run_update(run_id, "started", {"status": "running"})

        logger.info(f"🔵 Step 2: Getting run data for {run_id}")
        # Get run data to extract project_id
        query = """
        MATCH (r:Run {id: $run_id})
        RETURN r
        """
        result = await neo4j_client.execute_query(query, {"run_id": run_id})

        if not result:
            logger.error(f"🔴 Run not found in database: {run_id}")
            raise ValueError(f"Run not found: {run_id}")

        run_data = result[0]["r"]
        project_id = run_data.get("project_id")
        if not project_id:
            logger.error(f"🔴 Project ID not found for run: {run_id}")
            raise ValueError(f"Project ID not found for run: {run_id}")

        logger.info(f"🔵 Step 3: Importing MetaOrchestrator for run {run_id}")
        # Import MetaOrchestrator
        from auto_forge.application.orchestrator.meta_orchestrator import MetaOrchestrator

        logger.info(f"🔵 Step 4: Initializing orchestrator for run {run_id}")
        # Initialize orchestrator
        orchestrator = MetaOrchestrator(neo4j_client)

        # Initialize orchestrator (load default tasks)
        await orchestrator.initialize()

        # Start pipeline execution
        logger.info(
            f"🔵 Step 5: Starting pipeline execution for run: {run_id}, project: {project_id}"
        )

        # Broadcast pipeline starting event
        await broadcast_run_update(
            run_id,
            "pipeline_starting",
            {"status": "pipeline_starting", "project_id": str(project_id)},
        )

        logger.info(f"🔵 Step 6: Calling orchestrator.start_pipeline for run {run_id}")
        # Start pipeline using MetaOrchestrator
        context = await orchestrator.start_pipeline(
            project_id=UUID(project_id),
            run_id=UUID(run_id),
            pipeline_name="software_factory_v1",
        )

        logger.info(f"🔵 Step 7: Pipeline started successfully for run {run_id}, DAG ID: {context.dag_id}")

        # Broadcast pipeline started event
        await broadcast_run_update(
            run_id,
            "pipeline_started",
            {
                "status": "pipeline_started",
                "dag_id": str(context.dag_id),
                "total_nodes": context.total_nodes,
            },
        )

        # Execute pipeline
        logger.info(f"🔵 Step 8: Executing pipeline for run: {run_id}")
        result = await orchestrator.execute_pipeline(context)

        logger.info(f"🔵 Step 9: Pipeline execution completed for run {run_id}, result: {result}")

        # Update run status based on execution result
        execution_metrics = result.get("execution_metrics", {})

        if result.get("success", False):
            logger.info(f"🟢 Pipeline execution SUCCESS for run {run_id}")
            await neo4j_client.update_run_status(run_id, "completed")
            await broadcast_run_update(
                run_id,
                "completed",
                {
                    "status": "completed",
                    "total_tasks": execution_metrics.get("total_tasks", 0),
                    "completed_tasks": execution_metrics.get("completed_tasks", 0),
                    "failed_tasks": execution_metrics.get("failed_tasks", 0),
                    "total_execution_time": execution_metrics.get(
                        "total_execution_time", 0.0
                    ),
                },
            )
        else:
            logger.error(f"🔴 Pipeline execution FAILED for run {run_id}")
            await neo4j_client.update_run_status(
                run_id, "failed", error="Pipeline execution failed"
            )
            await broadcast_run_update(
                run_id,
                "failed",
                {
                    "status": "failed",
                    "error": "Pipeline execution failed",
                    "execution_metrics": execution_metrics,
                },
            )

    except Exception as e:
        logger.error(f"🔴 BACKGROUND TASK FAILED for run {run_id}: {str(e)}")
        logger.error(f"🔴 Exception type: {type(e).__name__}")
        import traceback
        logger.error(f"🔴 Traceback: {traceback.format_exc()}")

        try:
            # Try to update run status to failed
            await neo4j_client.update_run_status(
                run_id, "failed", error=f"Background task failed: {str(e)}"
            )
            await broadcast_run_update(
                run_id,
                "failed",
                {
                    "status": "failed",
                    "error": f"Background task failed: {str(e)}",
                },
            )
        except Exception as update_error:
            logger.error(f"🔴 Failed to update run status after background task failure: {update_error}")

        # Re-raise the exception to ensure FastAPI knows the background task failed
        raise


# Prometheus metrics
RUN_OPERATION_COUNT = Counter(
    "run_operation_total", "Total run operations", ["operation", "status"]
)
RUN_OPERATION_DURATION = Histogram(
    "run_operation_duration_seconds", "Run operation duration", ["operation"]
)

router = APIRouter()


async def get_neo4j_client() -> Neo4jClient:
    """Get Neo4j client instance."""
    client = Neo4jClient()
    await client.connect()
    return client


@router.post("/", response_model=RunResponse)
async def create_run(
    run_data: Dict[str, Any],
    background_tasks: BackgroundTasks,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
) -> RunResponse:
    """Create a new run and optionally start execution."""
    print(f"🔍 DEBUG: create_run called with: {run_data}")
    print(f"🔍 DEBUG: Type: {type(run_data)}")
    print(f"🔍 DEBUG: Keys: {list(run_data.keys())}")
    print(f"🔍 DEBUG: Has metadata: {'metadata' in run_data}")
    print(f"🔍 DEBUG: Metadata value: {run_data.get('metadata', 'NOT_FOUND')}")
    try:
        with RUN_OPERATION_DURATION.labels(operation="create").time():
            # Convert project_id to UUID if it's a string
            if "project_id" in run_data and isinstance(run_data["project_id"], str):
                run_data["project_id"] = UUID(run_data["project_id"])

            # Add default values for required fields
            from datetime import datetime, timezone
            from uuid import uuid4

            run_data.setdefault("id", str(uuid4()))
            run_data.setdefault("status", "running")
            run_data.setdefault("started_at", datetime.now(timezone.utc))
            run_data.setdefault("ended_at", None)
            run_data.setdefault("total_tasks", 0)
            run_data.setdefault("completed_tasks", 0)
            run_data.setdefault("successful_tasks", 0)
            run_data.setdefault("failed_tasks", 0)
            run_data.setdefault("total_tokens", 0)
            run_data.setdefault("total_cost", 0.0)
            run_data.setdefault("total_latency_ms", 0.0)
            run_data.setdefault("created_at", datetime.now(timezone.utc))
            run_data.setdefault("updated_at", datetime.now(timezone.utc))

            # Debug: Log before promotion
            print(f"🔍 DEBUG: Before promotion: {run_data}")
            print(f"🔍 DEBUG: Keys in run_data: {list(run_data.keys())}")
            print(f"🔍 DEBUG: Has metadata: {'metadata' in run_data}")
            logger.info(f"Before promotion: {run_data}")
            logger.info(f"Keys in run_data: {list(run_data.keys())}")
            logger.info(f"Type of run_data: {type(run_data)}")

            # Promote dict fields to *_json properties (only for fields that exist in Run model)
            promote = [
                ("metadata", "metadata_json"),
            ]
            for src, dst in promote:
                logger.info(f"Checking for {src} in run_data: {src in run_data}")
                if src in run_data:
                    logger.info(f"Promoting {src} to {dst}")
                    run_data[dst] = run_data.pop(src)
                else:
                    logger.info(f"Field {src} not found in run_data")

            # BULLETPROOF: Let the client handle all serialization
            # Just pass the data as-is, the client will handle everything

            # Debug: Log final run_data before calling create_run
            logger.info(f"ROUTE DEBUG: Final run_data before create_run: {run_data}")
            logger.info(
                f"ROUTE DEBUG: metadata_json type: {type(run_data.get('metadata_json'))}"
            )
            logger.info(
                f"ROUTE DEBUG: metadata_json value: {run_data.get('metadata_json')}"
            )

            # Debug: Log after promotion
            print(f"🔍 DEBUG: After promotion: {run_data}")
            logger.info(f"After promotion: {run_data}")

            # Write debug info to a file we can read
            # with open("debug_run_creation.log", "a") as f:
            #     f.write(f"DEBUG: After promotion: {run_data}\n")
            #     f.write(f"DEBUG: Keys: {list(run_data.keys())}\n")
            #     f.write(f"DEBUG: Has metadata_json: {'metadata_json' in run_data}\n")
            #     if "metadata_json" in run_data:
            #         f.write(
            #             f"DEBUG: metadata_json type: {type(run_data['metadata_json'])}\n"
            #         )
            #         f.write(
            #             f"DEBUG: metadata_json value: {run_data['metadata_json']}\n"
            #         )
            #     f.write("-" * 50 + "\n")

            # Create run with comprehensive error handling
            try:
                logger.info(f"🔍 About to create run in database")
                run_id = await neo4j_client.create_run(run_data)
                logger.info(f"🔍 Run created with ID: {run_id}")
            except Exception as run_error:
                logger.error(f"🔴 Failed to create run: {run_error}")
                import traceback
                logger.error(f"🔴 Run creation traceback: {traceback.format_exc()}")
                raise HTTPException(status_code=500, detail=f"Failed to create run: {run_error}")

            # Submit pipeline execution using existing agent tasks
            logger.info(f"🔍 About to submit task for run {run_id}")
            print(f"🔍 PRINT: About to submit task for run {run_id}")
            try:
                from auto_forge.infrastructure.workers.agent_tasks import execute_agent
                logger.info(f"🔍 Successfully imported execute_agent")
                print(f"🔍 PRINT: Successfully imported execute_agent")

                # Submit the pipeline execution as an agent task
                logger.info(f"🔍 About to call execute_agent.delay()")
                print(f"🔍 PRINT: About to call execute_agent.delay()")
                task = execute_agent.delay(
                    agent_type="planner",  # Start with planner agent
                    task_execution_data={
                        "run_id": run_id,
                        "project_id": str(run_data["project_id"]),
                        "pipeline_name": run_data["pipeline_name"],
                        "action": "start_pipeline"
                    },
                    project_id=str(run_data["project_id"]),
                    run_id=run_id,
                    node_id="pipeline_start"
                )

                logger.info(f"🔍 Successfully submitted task {task.id} for run {run_id}")
                print(f"🔍 PRINT: Successfully submitted task {task.id} for run {run_id}")
            except Exception as task_error:
                logger.error(f"🔴 Failed to submit task for run {run_id}: {task_error}")
                import traceback
                logger.error(f"🔴 Task submission traceback: {traceback.format_exc()}")
                # Continue without failing the run creation

            RUN_OPERATION_COUNT.labels(operation="create", status="success").inc()
            logger.info(f"Created run: {run_id}")

            # Broadcast run creation event
            await broadcast_run_update(
                str(run_id),
                "created",
                {
                    "project_name": run_data.get("project_name", "Unknown"),
                    "pipeline_type": run_data.get("pipeline_type", "unknown"),
                },
            )

            return RunResponse(
                run_id=UUID(run_id),
                status="created",
                message="Run created successfully",
            )

    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="create", status="error").inc()
        logger.error(f"Failed to create run: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create run: {e}")


@router.get("/{run_id}", response_model=Run)
async def get_run(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Run:
    """Get run by ID."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="get").time():
            query = """
            MATCH (r:Run {id: $run_id})-[:OF_PROJECT]->(p:Project)
            RETURN r, p.id as project_id
            """

            result = await neo4j_client.execute_query(query, {"run_id": str(run_id)})

            if not result:
                raise HTTPException(status_code=404, detail="Run not found")

            run_data = result[0]["r"]
            run_data["project_id"] = result[0]["project_id"]

            RUN_OPERATION_COUNT.labels(operation="get", status="success").inc()
            return Run(**run_data)

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="get", status="error").inc()
        logger.error(f"Failed to get run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get run: {e}")


@router.get("/", response_model=List[Run])
async def list_runs(
    project_id: Optional[UUID] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    status: Optional[str] = Query(None),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
) -> List[Run]:
    """List runs with pagination and filtering."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="list").time():
            # Build query with filters
            query = """
            MATCH (r:Run)-[:OF_PROJECT]->(p:Project)
            """
            parameters = {"limit": limit, "offset": offset}

            where_clauses = []

            if project_id:
                where_clauses.append("p.id = $project_id")
                parameters["project_id"] = str(project_id)

            if status:
                where_clauses.append("r.status = $status")
                parameters["status"] = status

            if where_clauses:
                query += " WHERE " + " AND ".join(where_clauses)

            query += """
            RETURN r, p.id as project_id
            ORDER BY r.started_at DESC
            SKIP $offset
            LIMIT $limit
            """

            result = await neo4j_client.execute_query(query, parameters)
            runs = []
            for record in result:
                run_data = record["r"]
                run_data["project_id"] = record["project_id"]
                runs.append(Run(**run_data))

            RUN_OPERATION_COUNT.labels(operation="list", status="success").inc()
            return runs

    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="list", status="error").inc()
        logger.error(f"Failed to list runs: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list runs: {e}")


@router.put("/{run_id}/status")
async def update_run_status(
    run_id: UUID,
    status: str,
    error: Optional[str] = None,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
) -> Dict[str, str]:
    """Update run status."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="update_status").time():
            # Check if run exists
            existing_run = await get_run(run_id, neo4j_client)
            if not existing_run:
                raise HTTPException(status_code=404, detail="Run not found")

            # Update status
            updates = {"error": error} if error else {}
            await neo4j_client.update_run_status(str(run_id), status, **updates)

            RUN_OPERATION_COUNT.labels(
                operation="update_status", status="success"
            ).inc()
            logger.info(f"Updated run {run_id} status to: {status}")

            return {"message": f"Run {run_id} status updated to {status}"}

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="update_status", status="error").inc()
        logger.error(f"Failed to update run {run_id} status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to update run status: {e}")


@router.delete("/{run_id}")
async def delete_run(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, str]:
    """Delete run by ID."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="delete").time():
            # Check if run exists
            existing_run = await get_run(run_id, neo4j_client)
            if not existing_run:
                raise HTTPException(status_code=404, detail="Run not found")

            # Delete run and all related data
            query = """
            MATCH (r:Run {id: $run_id})
            OPTIONAL MATCH (r)-[:PART_OF]->(t:Task)
            OPTIONAL MATCH (t)-[:GENERATES]->(a:Artifact)
            OPTIONAL MATCH (t)-[:RESULTED_IN]->(res:Result)
            OPTIONAL MATCH (t)<-[:EVALUATED_BY]-(c:Critique)
            OPTIONAL MATCH (t)<-[:FOR_TASK]-(d:Decision)
            DELETE a, res, c, d, t, r
            """

            await neo4j_client.execute_write_query(query, {"run_id": str(run_id)})

            RUN_OPERATION_COUNT.labels(operation="delete", status="success").inc()
            logger.info(f"Deleted run: {run_id}")

            return {"message": f"Run {run_id} deleted successfully"}

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="delete", status="error").inc()
        logger.error(f"Failed to delete run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete run: {e}")


@router.get("/{run_id}/summary")
async def get_run_summary(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """Get comprehensive summary of a run."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="get_summary").time():
            summary = await neo4j_client.get_run_summary(str(run_id))

            if not summary:
                raise HTTPException(status_code=404, detail="Run not found")

            RUN_OPERATION_COUNT.labels(operation="get_summary", status="success").inc()
            return summary

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="get_summary", status="error").inc()
        logger.error(f"Failed to get summary for run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get run summary: {e}")


@router.get("/{run_id}/status")
async def get_run_status(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """Get real-time run status with phase tracking (inspired by code_gen patterns)."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="status").time():
            run = await neo4j_client.get_run(run_id)
            if not run:
                raise HTTPException(status_code=404, detail="Run not found")

            # Get task records for this run
            tasks = await neo4j_client.get_run_tasks(run_id)

            # Calculate detailed statistics
            total_tasks = len(tasks)
            completed_tasks = len([t for t in tasks if t.get("status") == "completed"])
            failed_tasks = len([t for t in tasks if t.get("status") == "failed"])
            running_tasks = len([t for t in tasks if t.get("status") == "running"])

            # Calculate progress through development phases
            progress = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0

            # Development phases with weights (inspired by code_gen)
            phases = [
                {
                    "name": "PLAN",
                    "description": "Requirements Analysis",
                    "weight": 0.1,
                    "icon": "📋",
                },
                {
                    "name": "SPEC",
                    "description": "Technical Specification",
                    "weight": 0.15,
                    "icon": "📝",
                },
                {
                    "name": "ARCH",
                    "description": "System Architecture",
                    "weight": 0.2,
                    "icon": "🏗️",
                },
                {
                    "name": "CODE",
                    "description": "Code Generation",
                    "weight": 0.3,
                    "icon": "💻",
                },
                {
                    "name": "TEST",
                    "description": "Testing & Validation",
                    "weight": 0.15,
                    "icon": "🧪",
                },
                {
                    "name": "DEPLOY",
                    "description": "Deployment Ready",
                    "weight": 0.1,
                    "icon": "🚀",
                },
            ]

            # Determine current phase based on progress
            current_phase_index = 0
            cumulative_weight = 0
            for i, phase in enumerate(phases):
                cumulative_weight += phase["weight"]
                if progress <= cumulative_weight * 100:
                    current_phase_index = i
                    break

            current_phase = phases[current_phase_index]

            # Enhanced status response (inspired by code_gen)
            status_info = {
                "job_id": str(run_id),
                "status": run.get("status", "unknown"),
                "phase": current_phase["name"],
                "phase_description": current_phase["description"],
                "phase_icon": current_phase["icon"],
                "progress": round(progress, 2),
                "message": f"Currently in {current_phase['description']} phase",
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "failed_tasks": failed_tasks,
                "running_tasks": running_tasks,
                "started_at": run.get("started_at"),
                "ended_at": run.get("ended_at"),
                "pipeline_name": run.get("pipeline_name"),
                "project_id": run.get("project_id"),
                "error": run.get("error"),
                "artifacts_ready": run.get("status") == "completed",
                "real_time": True,
                "timestamp": run.get("updated_at"),
                "phases": phases,
                "current_phase_index": current_phase_index,
            }

            RUN_OPERATION_COUNT.labels(operation="status", status="success").inc()
            return status_info

    except Exception as e:
        logger.error(f"Failed to get run status: {e}")
        RUN_OPERATION_COUNT.labels(operation="status", status="error").inc()
        raise HTTPException(status_code=500, detail="Failed to get run status")


@router.get("/{run_id}/tasks", response_model=List[TaskRecord])
async def get_run_tasks(
    run_id: UUID,
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    status: Optional[str] = Query(None),
    agent: Optional[str] = Query(None),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
) -> List[TaskRecord]:
    """Get tasks for a specific run."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="get_tasks").time():
            # Build query with filters
            query = """
            MATCH (r:Run {id: $run_id})-[:PART_OF]->(t:Task)
            """
            parameters = {"run_id": str(run_id), "limit": limit, "offset": offset}

            where_clauses = []

            if status:
                where_clauses.append("t.status = $status")
                parameters["status"] = status

            if agent:
                where_clauses.append("t.agent = $agent")
                parameters["agent"] = agent

            if where_clauses:
                query += " WHERE " + " AND ".join(where_clauses)

            query += """
            RETURN t
            ORDER BY t.created_at ASC
            SKIP $offset
            LIMIT $limit
            """

            result = await neo4j_client.execute_query(query, parameters)
            tasks = [TaskRecord(**record["t"]) for record in result]

            RUN_OPERATION_COUNT.labels(operation="get_tasks", status="success").inc()
            return tasks

    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="get_tasks", status="error").inc()
        logger.error(f"Failed to get tasks for run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get run tasks: {e}")


@router.post("/{run_id}/cancel")
async def cancel_run(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, str]:
    """Cancel a running run."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="cancel").time():
            # Check if run exists and is running
            existing_run = await get_run(run_id, neo4j_client)
            if not existing_run:
                raise HTTPException(status_code=404, detail="Run not found")

            if existing_run.status not in ["running", "pending"]:
                raise HTTPException(
                    status_code=400, detail="Run is not in a cancellable state"
                )

            # Update run status to cancelled
            await neo4j_client.update_run_status(str(run_id), "cancelled")

            # Cancel running tasks in Celery
            from auto_forge.infrastructure.workers.agent_tasks import submit_agent_task
            import json

            # Get all running tasks for this run
            query = """
            MATCH (r:Run {id: $run_id})-[:PART_OF]->(t:Task)
            WHERE t.status IN ['running', 'pending']
            RETURN t.id as task_id, t.agent as agent_type
            """
            running_tasks = await neo4j_client.execute_query(
                query, {"run_id": str(run_id)}
            )

            # Cancel each running task
            for task_record in running_tasks:
                task_id = task_record["task_id"]
                agent_type = task_record["agent_type"]

                # Update task status to cancelled
                await neo4j_client.update_task_status(task_id, "cancelled")

                # Note: Celery doesn't have a direct cancel API, so we rely on status updates
                logger.info(f"Cancelled task {task_id} (agent: {agent_type})")

            RUN_OPERATION_COUNT.labels(operation="cancel", status="success").inc()
            logger.info(f"Cancelled run: {run_id}")

            return {"message": f"Run {run_id} cancelled successfully"}

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="cancel", status="error").inc()
        logger.error(f"Failed to cancel run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to cancel run: {e}")


@router.get("/{run_id}/artifacts")
async def get_run_artifacts(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """Get all artifacts produced by a run."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="get_artifacts").time():
            query = """
            MATCH (r:Run {id: $run_id})-[:PART_OF]->(t:Task)-[:GENERATES]->(a:Artifact)
            RETURN a
            ORDER BY a.created_at ASC
            """

            result = await neo4j_client.execute_query(query, {"run_id": str(run_id)})
            artifacts = [record["a"] for record in result]

            RUN_OPERATION_COUNT.labels(
                operation="get_artifacts", status="success"
            ).inc()

            return {
                "run_id": str(run_id),
                "artifacts": artifacts,
                "total_artifacts": len(artifacts),
                "artifact_types": list(set(artifact["type"] for artifact in artifacts)),
            }

    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="get_artifacts", status="error").inc()
        logger.error(f"Failed to get artifacts for run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get run artifacts: {e}")


@router.get("/{run_id}/pipeline-visualization")
async def get_pipeline_visualization(
    run_id: UUID, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """Get pipeline visualization data including stages, status, and dependencies."""
    try:
        with RUN_OPERATION_DURATION.labels(operation="get_pipeline_visualization").time():
            # Get run information
            run_query = """
            MATCH (r:Run {id: $run_id})
            RETURN r
            """
            run_result = await neo4j_client.execute_query(run_query, {"run_id": str(run_id)})

            if not run_result:
                raise HTTPException(status_code=404, detail="Run not found")

            run_data = run_result[0]["r"]

            # Get all tasks for this run with their status
            tasks_query = """
            MATCH (r:Run {id: $run_id})-[:PART_OF]->(t:Task)
            RETURN t
            ORDER BY t.created_at ASC
            """
            tasks_result = await neo4j_client.execute_query(tasks_query, {"run_id": str(run_id)})

            # Define pipeline stages from the software_factory_v1.yaml
            pipeline_stages = [
                {"id": "plan", "name": "Plan", "description": "Analyze requirements and create development plan", "agent": "planner", "position": {"x": 0, "y": 0}},
                {"id": "brainstorm", "name": "Brainstorm", "description": "Generate creative solutions and approaches", "agent": "planner", "position": {"x": 200, "y": 0}},
                {"id": "requirements", "name": "Requirements", "description": "Extract and formalize requirements", "agent": "specifier", "position": {"x": 400, "y": 0}},
                {"id": "spec", "name": "Specification", "description": "Create detailed technical specifications", "agent": "specifier", "position": {"x": 600, "y": 0}},
                {"id": "techspec", "name": "Tech Spec", "description": "Generate technology-specific specifications", "agent": "specifier", "position": {"x": 800, "y": 0}},
                {"id": "arch", "name": "Architecture", "description": "Design system architecture", "agent": "architect", "position": {"x": 1000, "y": 0}},
                {"id": "dataflow", "name": "Data Flow", "description": "Design data flow and integration patterns", "agent": "architect", "position": {"x": 1200, "y": 0}},
                {"id": "dbschema", "name": "DB Schema", "description": "Design database schema", "agent": "architect", "position": {"x": 1400, "y": 0}},
                {"id": "cicd", "name": "CI/CD", "description": "Design CI/CD pipeline", "agent": "architect", "position": {"x": 1600, "y": 0}},
                {"id": "scaffold", "name": "Scaffold", "description": "Create project structure and boilerplate", "agent": "coder", "position": {"x": 0, "y": 150}},
                {"id": "code", "name": "Code", "description": "Implement core functionality", "agent": "coder", "position": {"x": 200, "y": 150}},
                {"id": "testgen", "name": "Test Generation", "description": "Generate comprehensive test suites", "agent": "tester", "position": {"x": 400, "y": 150}},
                {"id": "testrun", "name": "Test Run", "description": "Execute test suites and collect results", "agent": "tester", "position": {"x": 600, "y": 150}},
                {"id": "assure", "name": "Assurance", "description": "Run assurance checks in parallel", "agent": "critic", "position": {"x": 800, "y": 150}},
                {"id": "perf_smoke", "name": "Performance Smoke", "description": "Run performance smoke tests", "agent": "tester", "position": {"x": 1000, "y": 150}},
                {"id": "critic", "name": "Critic", "description": "Evaluate overall quality and generate critique", "agent": "critic", "position": {"x": 1200, "y": 150}},
                {"id": "repair", "name": "Repair", "description": "Repair issues based on critique", "agent": "coder", "position": {"x": 1400, "y": 150}},
                {"id": "perf_opt", "name": "Performance Optimization", "description": "Optimize performance if needed", "agent": "coder", "position": {"x": 1600, "y": 150}},
                {"id": "finalize", "name": "Finalize", "description": "Package and document the solution", "agent": "coder", "position": {"x": 0, "y": 300}}
            ]

            # Define dependencies based on software_factory_v1.yaml
            dependencies = [
                {"from": "brainstorm", "to": "plan"},
                {"from": "requirements", "to": "plan"},
                {"from": "requirements", "to": "brainstorm"},
                {"from": "spec", "to": "requirements"},
                {"from": "techspec", "to": "spec"},
                {"from": "arch", "to": "spec"},
                {"from": "dataflow", "to": "arch"},
                {"from": "dbschema", "to": "arch"},
                {"from": "cicd", "to": "arch"},
                {"from": "scaffold", "to": "arch"},
                {"from": "scaffold", "to": "techspec"},
                {"from": "code", "to": "scaffold"},
                {"from": "testgen", "to": "code"},
                {"from": "testrun", "to": "testgen"},
                {"from": "assure", "to": "testrun"},
                {"from": "perf_smoke", "to": "testrun"},
                {"from": "critic", "to": "assure"},
                {"from": "critic", "to": "perf_smoke"},
                {"from": "repair", "to": "critic"},
                {"from": "perf_opt", "to": "critic"},
                {"from": "finalize", "to": "critic"},
                {"from": "finalize", "to": "repair"},
                {"from": "finalize", "to": "perf_opt"}
            ]

            # Map task status to stages
            stage_status = {}
            for task in tasks_result:
                task_data = task["t"]
                stage_name = task_data.get("stage", task_data.get("name", ""))
                if stage_name:
                    stage_status[stage_name] = {
                        "status": task_data.get("status", "pending"),
                        "start_time": task_data.get("start_time"),
                        "end_time": task_data.get("end_time"),
                        "duration": task_data.get("duration"),
                        "agent": task_data.get("agent"),
                        "task_id": task_data.get("id")
                    }

            # Update stages with current status
            for stage in pipeline_stages:
                stage_id = stage["id"]
                if stage_id in stage_status:
                    stage.update(stage_status[stage_id])
                else:
                    stage.update({
                        "status": "pending",
                        "start_time": None,
                        "end_time": None,
                        "duration": None,
                        "agent": stage["agent"],
                        "task_id": None
                    })

            RUN_OPERATION_COUNT.labels(
                operation="get_pipeline_visualization", status="success"
            ).inc()

            return {
                "run_id": str(run_id),
                "run_status": run_data.get("status", "unknown"),
                "run_created_at": run_data.get("created_at"),
                "run_updated_at": run_data.get("updated_at"),
                "stages": pipeline_stages,
                "dependencies": dependencies,
                "total_stages": len(pipeline_stages),
                "completed_stages": len([s for s in pipeline_stages if s["status"] == "completed"]),
                "running_stages": len([s for s in pipeline_stages if s["status"] == "running"]),
                "failed_stages": len([s for s in pipeline_stages if s["status"] == "failed"])
            }

    except HTTPException:
        raise
    except Exception as e:
        RUN_OPERATION_COUNT.labels(operation="get_pipeline_visualization", status="error").inc()
        logger.error(f"Failed to get pipeline visualization for run {run_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get pipeline visualization: {e}")
