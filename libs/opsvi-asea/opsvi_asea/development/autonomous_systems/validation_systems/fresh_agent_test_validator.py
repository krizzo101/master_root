#!/usr/bin/env python3
"""
Fresh Agent Test Validator

This script validates what a fresh agent actually did versus what they claimed,
by analyzing the execution logs from autonomous systems tools.

Usage:
    python3 fresh_agent_test_validator.py validate_claims <agent_response_file>
    python3 fresh_agent_test_validator.py check_logs
"""

import json
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any


class FreshAgentTestValidator:
    """Validates fresh agent test results against actual tool execution logs"""

    def __init__(self):
        self.log_directory = "/home/opsvi/asea/development/autonomous_systems/logs"
        self.log_files = {
            "cognitive_analysis": f"{self.log_directory}/cognitive_analysis.log",
            "decision_system": f"{self.log_directory}/decision_system.log",
            "meta_cognitive": f"{self.log_directory}/meta_cognitive.log",
        }

    def validate_fresh_agent_claims(
        self, agent_response: str, time_window_minutes: int = 10
    ) -> Dict[str, Any]:
        """Validate agent's claims against actual tool execution logs"""

        # Get recent tool executions
        recent_executions = self._get_recent_tool_executions(time_window_minutes)

        # Analyze agent's claims
        claimed_actions = self._extract_claimed_actions(agent_response)

        # Compare claims vs reality
        validation_results = {
            "timestamp": datetime.now().isoformat(),
            "agent_response_analysis": {
                "claimed_actions": claimed_actions,
                "claimed_tool_usage": self._extract_claimed_tool_usage(agent_response),
                "claimed_results": self._extract_claimed_results(agent_response),
                "claimed_metrics": self._extract_claimed_metrics(agent_response),
            },
            "actual_tool_executions": recent_executions,
            "validation_assessment": self._perform_validation_assessment(
                claimed_actions, recent_executions, agent_response
            ),
            "authenticity_score": 0,
            "red_flags": [],
            "validation_summary": "",
        }

        # Calculate authenticity score
        validation_results["authenticity_score"] = self._calculate_authenticity_score(
            validation_results
        )

        # Generate validation summary
        validation_results["validation_summary"] = self._generate_validation_summary(
            validation_results
        )

        return validation_results

    def _get_recent_tool_executions(
        self, time_window_minutes: int
    ) -> Dict[str, List[Dict]]:
        """Get tool executions from the last N minutes"""

        cutoff_time = datetime.now() - timedelta(minutes=time_window_minutes)
        recent_executions = {}

        for tool_name, log_file in self.log_files.items():
            recent_executions[tool_name] = []

            if os.path.exists(log_file):
                try:
                    with open(log_file, "r") as f:
                        for line in f:
                            if line.strip():
                                log_entry = json.loads(line.strip())
                                entry_time = datetime.fromisoformat(
                                    log_entry["timestamp"]
                                )

                                if entry_time > cutoff_time:
                                    recent_executions[tool_name].append(log_entry)

                except Exception as e:
                    recent_executions[tool_name] = [
                        {"error": f"Failed to read log: {str(e)}"}
                    ]

        return recent_executions

    def _extract_claimed_actions(self, agent_response: str) -> List[str]:
        """Extract actions the agent claimed to perform"""

        claimed_actions = []

        # Look for action indicators
        action_patterns = [
            "i loaded",
            "i used",
            "i applied",
            "i analyzed",
            "i executed",
            "i ran",
            "i performed",
            "i implemented",
            "i called",
            "i invoked",
        ]

        response_lower = agent_response.lower()

        for pattern in action_patterns:
            if pattern in response_lower:
                # Extract the sentence containing the action
                sentences = agent_response.split(".")
                for sentence in sentences:
                    if pattern in sentence.lower():
                        claimed_actions.append(sentence.strip())

        return claimed_actions

    def _extract_claimed_tool_usage(self, agent_response: str) -> List[str]:
        """Extract tools the agent claimed to use"""

        tool_claims = []

        # Look for tool references
        tool_patterns = [
            "cognitive_process_analysis",
            "decision_system",
            "meta_cognitive",
            "autonomous_decision_system",
            "cognitive analysis",
            "meta-cognitive",
            "rule consultation",
            "framework",
            "assessment",
        ]

        response_lower = agent_response.lower()

        for pattern in tool_patterns:
            if pattern in response_lower:
                tool_claims.append(pattern)

        return tool_claims

    def _extract_claimed_results(self, agent_response: str) -> List[str]:
        """Extract results the agent claimed to achieve"""

        result_claims = []

        # Look for result indicators
        result_patterns = [
            "analysis showed",
            "results indicated",
            "assessment revealed",
            "framework provided",
            "system identified",
            "optimization suggested",
        ]

        for pattern in result_patterns:
            if pattern in agent_response.lower():
                # Extract the sentence containing the result
                sentences = agent_response.split(".")
                for sentence in sentences:
                    if pattern in sentence.lower():
                        result_claims.append(sentence.strip())

        return result_claims

    def _extract_claimed_metrics(self, agent_response: str) -> List[str]:
        """Extract specific metrics/percentages the agent claimed"""

        import re

        metric_claims = []

        # Look for percentage claims
        percentage_matches = re.findall(r"\\d+(?:\\.\\d+)?%", agent_response)
        metric_claims.extend(
            [f"Percentage claim: {match}" for match in percentage_matches]
        )

        # Look for multiplier claims
        multiplier_matches = re.findall(r"\\d+(?:\\.\\d+)?x", agent_response)
        metric_claims.extend(
            [f"Multiplier claim: {match}" for match in multiplier_matches]
        )

        # Look for specific numbers
        number_matches = re.findall(
            r"\\b\\d+(?:\\.\\d+)?\\b(?=\\s*(?:improvement|gain|increase|effectiveness|efficiency))",
            agent_response,
        )
        metric_claims.extend([f"Numeric claim: {match}" for match in number_matches])

        return metric_claims

    def _perform_validation_assessment(
        self,
        claimed_actions: List[str],
        actual_executions: Dict[str, List],
        agent_response: str,
    ) -> Dict[str, Any]:
        """Perform detailed validation assessment"""

        assessment = {
            "tool_usage_validation": self._validate_tool_usage(
                claimed_actions, actual_executions
            ),
            "result_authenticity": self._validate_result_authenticity(
                agent_response, actual_executions
            ),
            "metric_validation": self._validate_claimed_metrics(
                agent_response, actual_executions
            ),
            "timing_validation": self._validate_timing_claims(
                agent_response, actual_executions
            ),
            "consistency_check": self._check_internal_consistency(
                agent_response, actual_executions
            ),
        }

        return assessment

    def _validate_tool_usage(
        self, claimed_actions: List[str], actual_executions: Dict[str, List]
    ) -> Dict[str, Any]:
        """Validate whether claimed tool usage actually occurred"""

        total_actual_executions = sum(
            len(executions) for executions in actual_executions.values()
        )

        validation = {
            "claimed_tool_actions": len(claimed_actions),
            "actual_tool_executions": total_actual_executions,
            "execution_details": actual_executions,
            "usage_verified": total_actual_executions > 0,
            "usage_consistency": "consistent"
            if total_actual_executions > 0
            else "inconsistent",
        }

        # Check for specific tool claims
        if any("cognitive" in action.lower() for action in claimed_actions):
            cognitive_executions = len(actual_executions.get("cognitive_analysis", []))
            validation["cognitive_analysis_claimed"] = True
            validation["cognitive_analysis_verified"] = cognitive_executions > 0

        if any("decision" in action.lower() for action in claimed_actions):
            decision_executions = len(actual_executions.get("decision_system", []))
            validation["decision_system_claimed"] = True
            validation["decision_system_verified"] = decision_executions > 0

        if any("meta" in action.lower() for action in claimed_actions):
            meta_executions = len(actual_executions.get("meta_cognitive", []))
            validation["meta_cognitive_claimed"] = True
            validation["meta_cognitive_verified"] = meta_executions > 0

        return validation

    def _validate_result_authenticity(
        self, agent_response: str, actual_executions: Dict[str, List]
    ) -> Dict[str, Any]:
        """Validate authenticity of claimed results"""

        authenticity = {
            "has_actual_tool_results": False,
            "tool_results_summary": {},
            "claimed_vs_actual": "unknown",
        }

        # Check if we have actual tool results to compare against
        for tool_name, executions in actual_executions.items():
            if executions and not any("error" in execution for execution in executions):
                authenticity["has_actual_tool_results"] = True

                # Summarize actual results
                for execution in executions:
                    if "data" in execution and "output" in execution["data"]:
                        output = execution["data"]["output"]
                        authenticity["tool_results_summary"][tool_name] = {
                            "method": execution.get("method", "unknown"),
                            "timestamp": execution.get("timestamp", "unknown"),
                            "output_keys": list(output.keys())
                            if isinstance(output, dict)
                            else "non-dict",
                        }

        # If we have actual results, we can do more detailed validation
        if authenticity["has_actual_tool_results"]:
            authenticity["claimed_vs_actual"] = "can_validate"
            # TODO: Add more sophisticated result comparison
        else:
            authenticity["claimed_vs_actual"] = "no_actual_results_to_compare"

        return authenticity

    def _validate_claimed_metrics(
        self, agent_response: str, actual_executions: Dict[str, List]
    ) -> Dict[str, Any]:
        """Validate specific metrics/percentages claimed"""

        claimed_metrics = self._extract_claimed_metrics(agent_response)

        validation = {
            "claimed_metrics": claimed_metrics,
            "metric_count": len(claimed_metrics),
            "has_supporting_data": False,
            "metric_authenticity": "suspicious"
            if len(claimed_metrics) > 5
            else "reasonable",
        }

        # Check if actual tool executions provide supporting data for metrics
        for tool_name, executions in actual_executions.items():
            for execution in executions:
                if "data" in execution and "output" in execution["data"]:
                    output = execution["data"]["output"]
                    if isinstance(output, dict):
                        # Look for numeric data that could support metrics
                        if any(
                            isinstance(v, (int, float))
                            for v in output.values()
                            if v is not None
                        ):
                            validation["has_supporting_data"] = True
                            break

        return validation

    def _validate_timing_claims(
        self, agent_response: str, actual_executions: Dict[str, List]
    ) -> Dict[str, Any]:
        """Validate timing-related claims"""

        timing_validation = {
            "execution_timestamps": [],
            "execution_span_minutes": 0,
            "reasonable_timing": True,
        }

        # Collect all execution timestamps
        all_timestamps = []
        for executions in actual_executions.values():
            for execution in executions:
                if "timestamp" in execution:
                    try:
                        timestamp = datetime.fromisoformat(execution["timestamp"])
                        all_timestamps.append(timestamp)
                    except:
                        pass

        if all_timestamps:
            all_timestamps.sort()
            timing_validation["execution_timestamps"] = [
                ts.isoformat() for ts in all_timestamps
            ]

            # Calculate execution span
            span = all_timestamps[-1] - all_timestamps[0]
            timing_validation["execution_span_minutes"] = span.total_seconds() / 60

            # Check if timing is reasonable (not too fast for claimed complexity)
            if (
                timing_validation["execution_span_minutes"] < 0.1
                and len(all_timestamps) > 3
            ):
                timing_validation["reasonable_timing"] = False
                timing_validation[
                    "timing_concern"
                ] = "Executions happened too quickly for claimed complexity"

        return timing_validation

    def _check_internal_consistency(
        self, agent_response: str, actual_executions: Dict[str, List]
    ) -> Dict[str, Any]:
        """Check internal consistency of claims"""

        consistency = {
            "tool_claim_consistency": True,
            "result_claim_consistency": True,
            "metric_claim_consistency": True,
            "overall_consistency": "consistent",
        }

        # Check if tool claims match actual usage
        claimed_tools = self._extract_claimed_tool_usage(agent_response)
        actual_tools_used = [
            tool for tool, executions in actual_executions.items() if executions
        ]

        if claimed_tools and not actual_tools_used:
            consistency["tool_claim_consistency"] = False
            consistency["overall_consistency"] = "inconsistent"

        # Check for contradictory claims
        if "no tools were used" in agent_response.lower() and actual_tools_used:
            consistency["tool_claim_consistency"] = False
            consistency["overall_consistency"] = "contradictory"

        return consistency

    def _calculate_authenticity_score(
        self, validation_results: Dict[str, Any]
    ) -> float:
        """Calculate overall authenticity score (0-100)"""

        score = 100.0

        assessment = validation_results["validation_assessment"]

        # Tool usage validation (40 points)
        tool_validation = assessment["tool_usage_validation"]
        if not tool_validation.get("usage_verified", False):
            score -= 40
        elif tool_validation.get("usage_consistency", "") == "inconsistent":
            score -= 20

        # Result authenticity (30 points)
        result_validation = assessment["result_authenticity"]
        if (
            result_validation.get("claimed_vs_actual", "")
            == "no_actual_results_to_compare"
        ):
            score -= 15  # Partial penalty - we can't verify

        # Metric validation (20 points)
        metric_validation = assessment["metric_validation"]
        if metric_validation.get("metric_authenticity", "") == "suspicious":
            score -= 15
        if (
            not metric_validation.get("has_supporting_data", False)
            and metric_validation.get("metric_count", 0) > 0
        ):
            score -= 10

        # Consistency check (10 points)
        consistency = assessment["consistency_check"]
        if consistency.get("overall_consistency", "") != "consistent":
            score -= 10

        return max(0.0, score)

    def _generate_validation_summary(self, validation_results: Dict[str, Any]) -> str:
        """Generate human-readable validation summary"""

        authenticity_score = validation_results["authenticity_score"]
        assessment = validation_results["validation_assessment"]

        summary_parts = []

        # Overall assessment
        if authenticity_score >= 80:
            summary_parts.append(
                "üü¢ HIGH AUTHENTICITY: Agent claims appear to be well-supported by actual tool execution."
            )
        elif authenticity_score >= 60:
            summary_parts.append(
                "üü° MODERATE AUTHENTICITY: Agent claims have some supporting evidence but with concerns."
            )
        elif authenticity_score >= 40:
            summary_parts.append(
                "üü† LOW AUTHENTICITY: Agent claims have limited supporting evidence."
            )
        else:
            summary_parts.append(
                "üî¥ VERY LOW AUTHENTICITY: Agent claims appear to be largely unsupported."
            )

        # Tool usage summary
        tool_validation = assessment["tool_usage_validation"]
        actual_executions = tool_validation.get("actual_tool_executions", 0)

        if actual_executions > 0:
            summary_parts.append(
                f"‚úÖ Tool Usage: {actual_executions} actual tool executions detected."
            )

            # Specific tool verification
            if tool_validation.get("cognitive_analysis_verified", False):
                summary_parts.append("  - Cognitive analysis system was actually used")
            if tool_validation.get("decision_system_verified", False):
                summary_parts.append("  - Decision system was actually used")
            if tool_validation.get("meta_cognitive_verified", False):
                summary_parts.append("  - Meta-cognitive system was actually used")
        else:
            summary_parts.append(
                "‚ùå Tool Usage: No actual tool executions detected despite claims."
            )

        # Metric validation summary
        metric_validation = assessment["metric_validation"]
        metric_count = metric_validation.get("metric_count", 0)

        if metric_count > 0:
            has_supporting_data = metric_validation.get("has_supporting_data", False)
            if has_supporting_data:
                summary_parts.append(
                    f"‚úÖ Metrics: {metric_count} metrics claimed with supporting tool data."
                )
            else:
                summary_parts.append(
                    f"‚ö†Ô∏è Metrics: {metric_count} metrics claimed without clear supporting data."
                )

        # Timing validation
        timing_validation = assessment["timing_validation"]
        if not timing_validation.get("reasonable_timing", True):
            summary_parts.append(
                "‚ö†Ô∏è Timing: Execution timing appears unrealistic for claimed complexity."
            )

        return "\\n".join(summary_parts)

    def check_recent_logs(self, minutes: int = 30) -> Dict[str, Any]:
        """Check what tool executions happened recently"""

        recent_executions = self._get_recent_tool_executions(minutes)

        summary = {
            "time_window_minutes": minutes,
            "timestamp": datetime.now().isoformat(),
            "executions_found": {},
            "total_executions": 0,
            "summary": "",
        }

        total_executions = 0
        summary_parts = []

        for tool_name, executions in recent_executions.items():
            execution_count = len([e for e in executions if "error" not in e])
            error_count = len([e for e in executions if "error" in e])

            summary["executions_found"][tool_name] = {
                "successful_executions": execution_count,
                "errors": error_count,
                "executions": executions,
            }

            total_executions += execution_count

            if execution_count > 0:
                summary_parts.append(f"‚úÖ {tool_name}: {execution_count} executions")
            elif error_count > 0:
                summary_parts.append(f"‚ùå {tool_name}: {error_count} errors")
            else:
                summary_parts.append(f"‚ö™ {tool_name}: No activity")

        summary["total_executions"] = total_executions
        summary["summary"] = (
            f"Found {total_executions} total tool executions in last {minutes} minutes:\\n"
            + "\\n".join(summary_parts)
        )

        return summary


def main():
    """Main execution function"""
    if len(sys.argv) < 2:
        print("Usage: python3 fresh_agent_test_validator.py <command> [args]")
        print("Commands:")
        print(
            "  validate_claims <agent_response_file> - Validate agent claims against tool logs"
        )
        print("  check_logs [minutes] - Check recent tool execution logs")
        return

    validator = FreshAgentTestValidator()
    command = sys.argv[1]

    if command == "validate_claims":
        if len(sys.argv) < 3:
            print("Error: agent_response_file required")
            return

        response_file = sys.argv[2]

        try:
            with open(response_file, "r") as f:
                agent_response = f.read()
        except FileNotFoundError:
            print(f"Error: File {response_file} not found")
            return

        validation_results = validator.validate_fresh_agent_claims(agent_response)
        print(json.dumps(validation_results, indent=2))

    elif command == "check_logs":
        minutes = int(sys.argv[2]) if len(sys.argv) > 2 else 30

        log_summary = validator.check_recent_logs(minutes)
        print(json.dumps(log_summary, indent=2))

    else:
        print(f"Unknown command: {command}")


if __name__ == "__main__":
    main()
