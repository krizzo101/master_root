"""Celery application configuration for the autonomous software factory."""

import logging
import os
from typing import Dict, Any
from celery import Celery
from celery.signals import task_prerun, task_postrun, task_failure, worker_init
from prometheus_client import Counter, Histogram, Gauge
from auto_forge.config.settings import Settings

# Initialize settings
settings = Settings()

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Prometheus metrics
TASKS_RUN = Counter(
    "celery_tasks_run_total", "Total number of tasks run", ["queue", "task_name"]
)
TASKS_FAILED = Counter(
    "celery_tasks_failed_total", "Total number of failed tasks", ["queue", "task_name"]
)
TASK_DURATION = Histogram(
    "celery_task_duration_seconds", "Task duration in seconds", ["queue", "task_name"]
)
ACTIVE_TASKS = Gauge(
    "celery_active_tasks", "Number of currently active tasks", ["queue"]
)

# Create Celery application
app = Celery(
    "software_factory",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=["workers.tasks", "workers.agent_tasks"],
)

# Celery configuration
app.conf.update(
    # Serialization
    task_serializer=settings.celery_task_serializer,
    result_serializer=settings.celery_result_serializer,
    accept_content=settings.celery_accept_content,
    # Task acknowledgment
    task_acks_late=settings.celery_task_acks_late,
    worker_prefetch_multiplier=settings.celery_worker_prefetch_multiplier,
    # Task routing
    task_routes=settings.celery_task_routes,
    # Result backend configuration
    result_expires=3600,  # 1 hour
    result_persistent=True,
    # Worker configuration
    worker_max_tasks_per_child=1000,
    worker_disable_rate_limits=False,
    # Retry configuration
    task_annotations={
        "*": {
            "retry_backoff": True,
            "retry_jitter": True,
            "max_retries": 3,
        }
    },
    # Redis-specific configuration
    broker_transport_options={
        "visibility_timeout": 3600,  # 1 hour
        "fanout_patterns": True,
        "queue_order_strategy": "priority",
    },
    result_backend_transport_options={
        "visibility_timeout": 3600,  # 1 hour
        "retry_policy": {"timeout": 5.0},
    },
    # Task result configuration
    task_ignore_result=False,
    task_store_errors_even_if_ignored=True,
    # Timezone
    timezone="UTC",
    enable_utc=True,
    # Beat schedule - only if beat is enabled
    beat_schedule={
        'health-check-every-5-minutes': {
            'task': 'workers.tasks.health_check',
            'schedule': 300.0,  # 5 minutes
            'options': {'queue': 'default'}
        },
    } if os.getenv('ENABLE_BEAT', 'true').lower() == 'true' else {},
    # Beat configuration
    beat_schedule_filename='celerybeat-schedule',
    beat_sync_every=1,
    beat_max_loop_interval=300,
)

# Define queue configuration
app.conf.task_routes.update(
    {
        "workers.tasks.default.*": {"queue": "default"},
        "workers.tasks.io.*": {"queue": "io"},
        "workers.tasks.heavy.*": {"queue": "heavy"},
        "workers.tasks.test.*": {"queue": "test"},
        "workers.agent_tasks.execute_planner_agent": {"queue": "default"},
        "workers.agent_tasks.execute_specifier_agent": {"queue": "default"},
        "workers.agent_tasks.execute_architect_agent": {"queue": "heavy"},
        "workers.agent_tasks.execute_coder_agent": {"queue": "heavy"},
        "workers.agent_tasks.execute_tester_agent": {"queue": "test"},
        "workers.agent_tasks.execute_critic_agent": {"queue": "default"},
        "workers.agent_tasks.execute_agent": {"queue": "default"},
    }
)

# Task routing for different queue types
app.conf.task_default_queue = "default"
app.conf.task_default_exchange = "software_factory"
app.conf.task_default_routing_key = "default"

# Queue definitions with priorities
app.conf.task_queues = {
    "default": {
        "exchange": "software_factory",
        "routing_key": "default",
        "queue_arguments": {"x-max-priority": 10},
    },
    "io": {
        "exchange": "software_factory",
        "routing_key": "io",
        "queue_arguments": {"x-max-priority": 10},
    },
    "heavy": {
        "exchange": "software_factory",
        "routing_key": "heavy",
        "queue_arguments": {"x-max-priority": 10},
    },
    "test": {
        "exchange": "software_factory",
        "routing_key": "test",
        "queue_arguments": {"x-max-priority": 10},
    },
}

# Import task modules to ensure they are registered
import workers.tasks
import workers.agent_tasks

# Test task
@app.task(name="workers.celery_app.test_task")
def test_task():
    """Simple test task to verify task registration."""
    return {"status": "success", "message": "Test task executed"}

# Pipeline execution task
@app.task(bind=True, name="workers.tasks.execute_pipeline")
def execute_pipeline(self, run_id: str, project_id: str, pipeline_name: str = "software_factory_v1"):
    """Execute a pipeline using the MetaOrchestrator."""
    import asyncio
    import logging

    logger = logging.getLogger(__name__)
    logger.info(f"ðŸš€ CELERY PIPELINE TASK STARTED for run: {run_id}")

    try:
        # Import MetaOrchestrator
        from auto_forge.application.orchestrator.meta_orchestrator import MetaOrchestrator
        from memory.graph.client import Neo4jClient

        logger.info(f"ðŸš€ Step 1: Initializing Neo4j client for run {run_id}")
        # Initialize Neo4j client
        neo4j_client = Neo4jClient()

        logger.info(f"ðŸš€ Step 2: Initializing orchestrator for run {run_id}")
        # Initialize orchestrator
        orchestrator = MetaOrchestrator(neo4j_client)

        logger.info(f"ðŸš€ Step 3: Starting pipeline for run {run_id}, project {project_id}")
        # Start pipeline using MetaOrchestrator
        context = asyncio.run(orchestrator.start_pipeline(
            project_id=project_id,
            run_id=run_id,
            pipeline_name=pipeline_name,
        ))

        logger.info(f"ðŸš€ Step 4: Pipeline started successfully for run {run_id}, DAG ID: {context.dag_id}")

        logger.info(f"ðŸš€ Step 5: Executing pipeline for run {run_id}")
        # Execute pipeline
        result = asyncio.run(orchestrator.execute_pipeline(context))

        logger.info(f"ðŸš€ Step 6: Pipeline execution completed for run {run_id}, result: {result}")

        if result.get("success", False):
            logger.info(f"ðŸŸ¢ Pipeline execution SUCCESS for run {run_id}")
            return {
                "status": "success",
                "run_id": run_id,
                "result": result,
            }
        else:
            logger.error(f"ðŸ”´ Pipeline execution FAILED for run {run_id}")
            return {
                "status": "failed",
                "run_id": run_id,
                "error": "Pipeline execution failed",
                "result": result,
            }

    except Exception as e:
        logger.error(f"ðŸ”´ CELERY PIPELINE TASK FAILED for run {run_id}: {str(e)}")
        import traceback
        logger.error(f"ðŸ”´ Traceback: {traceback.format_exc()}")

        return {
            "status": "failed",
            "run_id": run_id,
            "error": str(e),
        }

# Celery signals for monitoring and metrics
@worker_init.connect
def worker_init_handler(sender=None, **kwargs):
    """Initialize worker with logging and metrics."""
    logger.info(f"Worker {sender} initialized")
    logger.info(f"Broker URL: {settings.celery_broker_url}")
    logger.info(f"Result Backend: {settings.celery_result_backend}")
    logger.info(f"Available queues: {list(app.conf.task_queues.keys())}")


@task_prerun.connect
def task_prerun_handler(
    sender=None, task_id=None, task=None, args=None, kwargs=None, **extras
):
    """Handle task pre-run events."""
    queue = task.request.delivery_info.get("routing_key", "unknown")
    task_name = task.name

    # Log task start with correlation IDs
    logger.info(
        f"Task started: {task_name}",
        extra={
            "task_id": task_id,
            "queue": queue,
            "project_id": kwargs.get("project_id"),
            "run_id": kwargs.get("run_id"),
            "node_id": kwargs.get("node_id"),
        },
    )

    # Update metrics
    ACTIVE_TASKS.labels(queue=queue).inc()

    # Store start time for duration calculation
    import time

    task.request.start_time = time.time()


@task_postrun.connect
def task_postrun_handler(
    sender=None,
    task_id=None,
    task=None,
    args=None,
    kwargs=None,
    retval=None,
    state=None,
    **extras,
):
    """Handle task post-run events."""
    queue = task.request.delivery_info.get("routing_key", "unknown")
    task_name = task.name

    # Calculate task duration
    import time

    if hasattr(task.request, "start_time"):
        duration = time.time() - task.request.start_time
        TASK_DURATION.labels(queue=queue, task_name=task_name).observe(duration)

    # Update metrics
    TASKS_RUN.labels(queue=queue, task_name=task_name).inc()
    ACTIVE_TASKS.labels(queue=queue).dec()

    # Log task completion
    logger.info(
        f"Task completed: {task_name} - State: {state}",
        extra={
            "task_id": task_id,
            "queue": queue,
            "state": state,
            "project_id": kwargs.get("project_id"),
            "run_id": kwargs.get("run_id"),
            "node_id": kwargs.get("node_id"),
        },
    )


@task_failure.connect
def task_failure_handler(
    sender=None,
    task_id=None,
    exception=None,
    args=None,
    kwargs=None,
    traceback=None,
    einfo=None,
    **extras,
):
    """Handle task failure events."""
    queue = sender.request.delivery_info.get("routing_key", "unknown")
    task_name = sender.name

    # Update failure metrics
    TASKS_FAILED.labels(queue=queue, task_name=task_name).inc()
    ACTIVE_TASKS.labels(queue=queue).dec()

    # Log task failure
    logger.error(
        f"Task failed: {task_name} - Exception: {exception}",
        extra={
            "task_id": task_id,
            "queue": queue,
            "exception": str(exception),
            "project_id": kwargs.get("project_id"),
            "run_id": kwargs.get("run_id"),
            "node_id": kwargs.get("node_id"),
        },
        exc_info=True,
    )


# Health check task
@app.task(bind=True, name="workers.tasks.health_check")
def health_check(self):
    """Health check task to verify Celery is working."""
    import time

    logger.info(f"Health check task {self.request.id} executed")
    return {
        "status": "healthy",
        "task_id": self.request.id,
        "timestamp": time.time(),
    }


# Test task
@app.task(bind=True, name="workers.tasks.test_task")
def test_task(self):
    """Test task to verify task registration."""
    import time

    logger.info(f"Test task {self.request.id} executed")
    return {
        "status": "success",
        "task_id": self.request.id,
        "timestamp": time.time(),
        "message": "Test task executed",
    }


# Export the Celery app
__all__ = ["app"]
